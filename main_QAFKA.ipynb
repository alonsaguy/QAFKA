{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impossible-behavior",
   "metadata": {},
   "source": [
    "# QAFKA\n",
    "\n",
    "## Welcome to QAFKA jupyter notebook\n",
    "\n",
    "\n",
    "In order to run QAFKA please follow the instructions in the following blocks.\n",
    "For more information please visit [link to paper](https://www.google.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-landing",
   "metadata": {},
   "source": [
    "### Parameters Initialization and Setup\n",
    "\n",
    "In the next block we will import the relevant packages for QAFKA.\n",
    "Then, we will define some important parameters for our run.\n",
    "\n",
    "**numOfBins** - Specifies the number of bins in the histogram of the number of blinking events (default: 20)\n",
    "\n",
    "**chop** - Specifies which frames of the experiment you would like to analyze. For example: chop = \\[0, 1000\\] will cause QAFKA to analyze the experiment between the first frame and the 1000th frame.\n",
    "\n",
    "**pixel_length** - Specifies the experiment's pixel size \\[nm\\]\n",
    "\n",
    "**scale_size** - Specifies the resolution scaling. For example: scale_size = 3 and pixel_length = 150 \\[nm\\] would results in a reconstrcuted image with grid size of 50 \\[nm\\].\n",
    "\n",
    "**emitters_size** - Specifies the emitters merging radius for detection. If two clusters would be located within this radius they would be considered as a single cluster.\n",
    "\n",
    "**numOfClusters** - Specifies the number of simulated clusters in each simulated experiment\n",
    "\n",
    "**dataPath** - Specifies the path to the data.\n",
    "\n",
    "**file_names** - Specifies the names of the TIFF files (at least one experiment is required). For example: 'first_exp.tif'.\n",
    "\n",
    "**qualityThreshold** - Specifies the minimal fitting score for the localization block. (default: 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "amazing-tragedy",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiffcapture'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7d201ef9ec09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdataloaders\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mneural_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtrainers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bodic\\Jupyter QAFKA\\datasets.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtiffcapture\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tiffcapture'"
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "from dataloaders import *\n",
    "from neural_network import *\n",
    "from trainers import *\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "numOfBins = 20\n",
    "chop = [0, 12000]\n",
    "pixel_length = 150 #[nm]\n",
    "scale_size = 3\n",
    "emitters_size = 150 #[nm]\n",
    "file_names = [r'Ligand/unstim/200327_METmEos4b_C6_unstim_S3_cell2_568.tif']\n",
    "qualityThreshold = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-judge",
   "metadata": {},
   "source": [
    "### Run Configuration\n",
    "\n",
    "**LoadData** - Determines if we want to load new experimental data (True) or we want to use an already loaded data (False).\n",
    "\n",
    "**FilterBeads** - Determines if an additional beads filtration algorithm is needed for the experimental data.\n",
    "\n",
    "**CreateSimulatedData** - Determines if we want to use the same training data as before (True) or we want to create new training set (False).\n",
    "\n",
    "**TrainNet** - Determines if we want to train the neural network (True) or not (False).\n",
    "\n",
    "**preTrainedModel** - Specifies the pre-trained model to load in case we do not want to train the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "occupational-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoadData = True\n",
    "FilterBeads = False\n",
    "CreateSimulatedData = True\n",
    "TrainNet = True\n",
    "preTrainedModel = 'model_final_gauss'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-correlation",
   "metadata": {},
   "source": [
    "### Analysis Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "satisfactory-substitute",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8f9f27913ee3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmax_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mchop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mresolution_nm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpixel_length\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mscale_size\u001b[0m \u001b[1;31m#[nm]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCreateData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtrajectories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclusterCoordinations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chop' is not defined"
     ]
    }
   ],
   "source": [
    "max_size = int((chop[1]-chop[0])/1000)\n",
    "resolution_nm = pixel_length/scale_size #[nm]\n",
    "\n",
    "if(CreateData):\n",
    "    trajectories, clusterCoordinations = [], []\n",
    "    for i, file in enumerate(file_names):\n",
    "        print(\"**** Analyzing Tiff number {} ****\".format(i+1))\n",
    "        # Load TIFF files and create data_set\n",
    "        Data_Set = CreateDataSet(file, chop)\n",
    "        \n",
    "        # Segment the experiment before and after laser activation\n",
    "        seg = segment(Data_Set, threshold=0.15, window_size=100)\n",
    "        \n",
    "        # Filter beads (if True)\n",
    "        if(FilterBeads):\n",
    "            Data_Set = Filter_beads(Data_Set)\n",
    "        \n",
    "        # Background noise cleaning\n",
    "        Data_Set = clean_bg_noise(Data_Set, patch_length=5)\n",
    "        \n",
    "        # Clusters localization\n",
    "        Max_Data_Set = CreateMaxDataSet(Data_Set, max_size, seg)\n",
    "        DataThreshold, MaxThreshold = calc_threshold(Data_Set, Max_Data_Set)\n",
    "        coordinates = LocalizeEmitters(Max_Data_Set, MaxThreshold, quality_threshold, pixel_length, resolution_nm, emitters_size)\n",
    "        \n",
    "        # Create time traces for each cluster\n",
    "        timeTraces = ExtractTimeTraces(Data_Set[seg:, :, :], coordinates, pixel_length, resolution_nm, quality_threshold, DataThreshold, emitters_size)\n",
    "        \n",
    "        # Save the time traces and clusters locations of all experiments in a list\n",
    "        trajectories.append(timeTraces)\n",
    "        clusterCoordinations.append(coordinates)\n",
    "        \n",
    "        # The coordinations file would be saved as 'coordinated.npy'\n",
    "        np.save('coordinates', coordinates)\n",
    "\n",
    "    # Extract the features that would serve as the neural network's input\n",
    "    X_test = feature_extraction(trajectories, DataThreshold, numOfBins)\n",
    "else:\n",
    "    # Load features of an already analyzed experiment\n",
    "    X_test = LoadFinalDataSet()\n",
    "\n",
    "print(\"-I- Experimental Data was loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-queen",
   "metadata": {},
   "source": [
    "### Visualize Localizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "great-ferry",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'debug_entire_exp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-4eecd7076171>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# This will plot a max projection image of the last experiment with the localization marked on it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdebug_entire_exp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMax_Data_Set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'debug_entire_exp' is not defined"
     ]
    }
   ],
   "source": [
    "# This will plot a max projection image of the last experiment with the localization marked on it\n",
    "debug_entire_exp(Max_Data_Set, coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-harvey",
   "metadata": {},
   "source": [
    "### Simulated Data Setup\n",
    "\n",
    "If you chose to simulate the training data, you would need to specify the following parameters:\n",
    "\n",
    "**numOfClusters** - Specifies the number of simulated clusters in each simulation (relevant only if CreateSimulatedData is set to True).\n",
    "\n",
    "**bleach_proba** - Specifies the bleaching probability of the used fluorophore.\n",
    "\n",
    "**TrainSetSize** - Specifies the number of simulated experiments to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "automated-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfClusters = 200\n",
    "bleach_proba = 0.41\n",
    "TrainSetSize = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-muscle",
   "metadata": {},
   "source": [
    "### Create Simulated Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "affected-premiere",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CreateSimulatedDataSet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-09a34b02189e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCreateSimulatedData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCreateSimulatedDataSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrainSetSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_molecules\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbleach_proba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumOfBins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoadSimulatedDataSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CreateSimulatedDataSet' is not defined"
     ]
    }
   ],
   "source": [
    "if(CreateSimulatedData):\n",
    "    [X, y] = CreateSimulatedDataSet(TrainSetSize, num_of_molecules, bleach_proba, numOfBins)\n",
    "else:\n",
    "    [X, y] = LoadSimulatedDataSet()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.75)\n",
    "[X_train, X_val, X_test] = Normalization(X_train, X_val, X_test)\n",
    "[X_train, X_val, X_test] = BiasTrick(X_train, X_val, X_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_val = torch.FloatTensor(y_val)\n",
    "\n",
    "print(\"-I- Simulated Data was created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-carpet",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "In the next block we will build the neural network model.\n",
    "\n",
    "**lr** - Specifies the training phase learning rate.\n",
    "\n",
    "**betas** - Specifies the parameters for ADAM optimizer.\n",
    "\n",
    "**batch_size** - Specifies the batch size of the training phase.\n",
    "\n",
    "**epochs** - Specifies the maximal training epoch.\n",
    "\n",
    "**early_stopping** - Specifies the tolerance of the neural network to lack of improvement in the validation loss. For example: early_stopping = 5, would stop the trainig phase if the validation loss did not improve for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "better-winner",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-33d8a980f284>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCustomNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "lr = 1e-5\n",
    "betas = (0.99, 0.999)\n",
    "batch_size = 4\n",
    "epochs = 1000\n",
    "early_stopping = np.min((int(epochs/5), 15))\n",
    "\n",
    "model = CustomNet(torch.numel(X_train[0]), [128, 128, 128, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-calgary",
   "metadata": {},
   "source": [
    "### Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "unknown-anniversary",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-4e28f1e789cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrainNet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbetas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdl_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCreateDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "if(TrainNet):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "    dl_train = CreateDataLoader(X_train, y_train, batch_size=batch_size)\n",
    "    dl_val = CreateDataLoader(X_val, y_val, batch_size=1)\n",
    "\n",
    "    # ================= Train Net ================\n",
    "    trainer = Trainer(model, criterion, optimizer)\n",
    "    trainer.fit(dl_train, dl_val, num_epochs=epochs, early_stopping=early_stopping, print_every=1)\n",
    "    torch.save(trainer.model.state_dict(), 'model_final_gauss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-force",
   "metadata": {},
   "source": [
    "### Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cutting-marker",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-4854a0eab39d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreTrainedModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(preTrainedModel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-climate",
   "metadata": {},
   "source": [
    "### Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "wooden-disclaimer",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-1ba0a95bfaff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_val_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_val_pred = model(X_val)\n",
    "y_test_pred = model(X_test).squeeze()\n",
    "y_test_pred = torch.max(y_test_pred, torch.zeros(y_test_pred.shape))\n",
    "\n",
    "val_acc = torch.mean(torch.abs(y_val_pred.squeeze() - y_val))\n",
    "print(\"Neural Network Validation MSE:\", 100 * val_acc.item())\n",
    "\n",
    "print(\"Dimers Percentage Predictions Per Experiment:\")\n",
    "if(y_test_pred.shape == torch.Size([])):\n",
    "    print(\"0:\", 100 * y_test_pred.item())\n",
    "else:\n",
    "    for i in range(y_test_pred.shape[0]):\n",
    "        print(\"i:\", 100 * y_test_pred[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-omega",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-moses",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-prediction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
