{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impossible-behavior",
   "metadata": {},
   "source": [
    "# QAFKA\n",
    "\n",
    "## Welcome to QAFKA jupyter notebook\n",
    "\n",
    "\n",
    "In order to run QAFKA please follow the instructions in the following blocks.\n",
    "For more information please visit [link to paper](https://www.google.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-teddy",
   "metadata": {},
   "source": [
    "### Software Installation\n",
    "In the next block we will import the relevant packages for QAFKA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "amazing-tragedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiffcapture in d:\\miniconda\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: numpy>=1.8.0 in d:\\miniconda\\lib\\site-packages (from tiffcapture) (1.19.2)\n",
      "Requirement already satisfied: Pillow>=2.3.1 in d:\\miniconda\\lib\\site-packages (from tiffcapture) (8.1.2)\n",
      "Requirement already satisfied: torch in d:\\miniconda\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\miniconda\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in d:\\miniconda\\lib\\site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: ipympl in d:\\miniconda\\lib\\site-packages (0.6.3)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in d:\\miniconda\\lib\\site-packages (from ipympl) (3.3.4)\n",
      "Requirement already satisfied: ipywidgets>=7.6.0 in d:\\miniconda\\lib\\site-packages (from ipympl) (7.6.3)\n",
      "Requirement already satisfied: ipykernel>=4.7 in d:\\miniconda\\lib\\site-packages (from ipympl) (5.3.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\miniconda\\lib\\site-packages (from matplotlib>=2.0.0->ipympl) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\miniconda\\lib\\site-packages (from matplotlib>=2.0.0->ipympl) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in d:\\miniconda\\lib\\site-packages (from matplotlib>=2.0.0->ipympl) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\miniconda\\lib\\site-packages (from matplotlib>=2.0.0->ipympl) (8.1.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\miniconda\\lib\\site-packages (from matplotlib>=2.0.0->ipympl) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.15 in d:\\miniconda\\lib\\site-packages (from matplotlib>=2.0.0->ipympl) (1.19.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in d:\\miniconda\\lib\\site-packages (from ipywidgets>=7.6.0->ipympl) (5.1.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\miniconda\\lib\\site-packages (from ipywidgets>=7.6.0->ipympl) (5.0.5)Packages installation completed successfully\n",
      "\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in d:\\miniconda\\lib\\site-packages (from ipywidgets>=7.6.0->ipympl) (3.5.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in d:\\miniconda\\lib\\site-packages (from ipywidgets>=7.6.0->ipympl) (1.0.0)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in d:\\miniconda\\lib\\site-packages (from ipywidgets>=7.6.0->ipympl) (7.20.0)\n",
      "Requirement already satisfied: tornado>=4.2 in d:\\miniconda\\lib\\site-packages (from ipykernel>=4.7->ipympl) (6.1)\n",
      "Requirement already satisfied: jupyter-client in d:\\miniconda\\lib\\site-packages (from ipykernel>=4.7->ipympl) (6.1.11)\n",
      "Requirement already satisfied: six>=1.5 in d:\\miniconda\\lib\\site-packages (from python-dateutil>=2.1->matplotlib>=2.0.0->ipympl) (1.15.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in d:\\miniconda\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>=7.6.0->ipympl) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in d:\\miniconda\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>=7.6.0->ipympl) (4.7.1)\n",
      "Requirement already satisfied: ipython-genutils in d:\\miniconda\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>=7.6.0->ipympl) (0.2.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in d:\\miniconda\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (6.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in d:\\miniconda\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipympl) (49.2.1)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in d:\\miniconda\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipympl) (0.4.4)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\miniconda\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipympl) (0.18.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in d:\\miniconda\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipympl) (3.0.14)\n",
      "Requirement already satisfied: backcall in d:\\miniconda\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipympl) (0.2.0)\n",
      "Requirement already satisfied: decorator in d:\\miniconda\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipympl) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in d:\\miniconda\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipympl) (0.7.5)\n",
      "Requirement already satisfied: pygments in d:\\miniconda\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipympl) (2.7.4)\n",
      "Requirement already satisfied: pyzmq>=13 in d:\\miniconda\\lib\\site-packages (from jupyter-client->ipykernel>=4.7->ipympl) (22.0.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in d:\\miniconda\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.6.0->ipympl) (20.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in d:\\miniconda\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.6.0->ipympl) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in d:\\miniconda\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.6.0->ipympl) (3.4.0)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" in d:\\miniconda\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets>=7.6.0->ipympl) (227)\n",
      "Requirement already satisfied: jinja2 in d:\\miniconda\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (2.11.3)\n",
      "Requirement already satisfied: argon2-cffi in d:\\miniconda\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (20.1.0)\n",
      "Requirement already satisfied: nbconvert in d:\\miniconda\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (6.0.7)\n",
      "Requirement already satisfied: terminado>=0.8.3 in d:\\miniconda\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (0.9.2)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in d:\\miniconda\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (1.5.0)\n",
      "Requirement already satisfied: prometheus-client in d:\\miniconda\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (0.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in d:\\miniconda\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipympl) (0.8.1)\n",
      "Requirement already satisfied: wcwidth in d:\\miniconda\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.6.0->ipympl) (0.2.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in d:\\miniconda\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.6.0->ipympl) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\miniconda\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.6.0->ipympl) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\miniconda\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (1.1.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in d:\\miniconda\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (1.13.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in d:\\miniconda\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (0.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in d:\\miniconda\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (0.8.4)\n",
      "Requirement already satisfied: bleach in d:\\miniconda\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (3.3.0)\n",
      "Requirement already satisfied: testpath in d:\\miniconda\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (0.4.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in d:\\miniconda\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in d:\\miniconda\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (0.6.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in d:\\miniconda\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (0.5.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in d:\\miniconda\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (0.1.2)\n",
      "Requirement already satisfied: pywinpty>=0.5; os_name == \"nt\" in d:\\miniconda\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (0.5.7)\n",
      "Requirement already satisfied: pycparser in d:\\miniconda\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (2.19)\n",
      "Requirement already satisfied: packaging in d:\\miniconda\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (20.9)\n",
      "Requirement already satisfied: webencodings in d:\\miniconda\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (0.5.1)\n",
      "Requirement already satisfied: async-generator in d:\\miniconda\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in d:\\miniconda\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.0->ipympl) (1.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiffcapture\n",
    "!pip install torch\n",
    "!pip install ipympl\n",
    "\n",
    "from datasets import *\n",
    "from dataloaders import *\n",
    "from neural_network import *\n",
    "from trainers import *\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Packages installation completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-landing",
   "metadata": {},
   "source": [
    "### Parameters Initialization\n",
    "\n",
    "Now you should specify some important parameters for your run.\n",
    "\n",
    "**numOfBins** - Specifies the number of bins in the histogram of the number of blinking events (default: 20)\n",
    "\n",
    "**chop** - Specifies which frames of the experiment you would like to analyze. For example: chop = \\[0, 1000\\] will cause QAFKA to analyze the experiment between the first frame and the 1000th frame.\n",
    "\n",
    "**pixel_length** - Specifies the experiment's pixel size \\[nm\\]\n",
    "\n",
    "**scale_size** - Specifies the resolution scaling. For example: scale_size = 3 and pixel_length = 150 \\[nm\\] would results in a reconstrcuted image with grid size of 50 \\[nm\\].\n",
    "\n",
    "**emitters_size** - Specifies the emitters merging radius for detection. If two clusters would be located within this radius they would be considered as a single cluster.\n",
    "\n",
    "**numOfClusters** - Specifies the number of simulated clusters in each simulated experiment\n",
    "\n",
    "**file_names** - Specifies the names of the TIFF files (at least one experiment is required). For example: 'first_exp.tif'.\n",
    "\n",
    "**qualityThreshold** - Specifies the minimal fitting score for the localization block. (default: 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "surprised-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfBins = 20\n",
    "chop = [0, 2000]\n",
    "pixel_length = 157 #[nm]\n",
    "scale_size = 3\n",
    "merging_radius = 50 #[nm]\n",
    "file_names = [r'D:\\Project\\data\\CTLA4\\mEos3.2.tif', r'D:\\Project\\data\\CTLA4\\mEos3.2 (1).tif']\n",
    "qualityThreshold = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-judge",
   "metadata": {},
   "source": [
    "### Run Configuration\n",
    "\n",
    "**LoadData** - Determines if we want to load new experimental data (True) or we want to use an already loaded data (False).\n",
    "\n",
    "**FilterBeads** - Determines if an additional beads filtration algorithm is needed for the experimental data.\n",
    "\n",
    "**CreateSimulatedData** - Determines if we want to use the same training data as before (True) or we want to create new training set (False).\n",
    "\n",
    "**TrainNet** - Determines if we want to train the neural network (True) or not (False).\n",
    "\n",
    "**preTrainedModel** - Specifies the pre-trained model to load in case we do not want to train the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "occupational-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoadData = True\n",
    "FilterBeads = False\n",
    "CreateSimulatedData = True\n",
    "TrainNet = True\n",
    "preTrainedModel = 'model_final_gauss'\n",
    "# Add training time of the net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-correlation",
   "metadata": {},
   "source": [
    "### Analysis Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "satisfactory-substitute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Analyzing Tiff number 1 ****\n",
      "-I- Found segmentation in frame: 240\n",
      "-I- Background noise was filtered\n",
      "Emitter is out of bound: 439\n",
      "Bad fitting grade: 11\n",
      "Emitters intensity is too low: 44\n",
      "-I- found 105 emitters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MiniConda\\lib\\site-packages\\scipy\\optimize\\minpack.py:829: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blink too far from emitters: 5745\n",
      "Emitter is out of bound: 121\n",
      "Bad fitting grade: 40\n",
      "Emitters intensity is too low: 4518\n",
      "-I- updated emitters time traces\n",
      "**** Analyzing Tiff number 2 ****\n",
      "-I- Found segmentation in frame: 266\n",
      "-I- Background noise was filtered\n",
      "Emitter is out of bound: 9\n",
      "Bad fitting grade: 20\n",
      "Emitters intensity is too low: 36\n",
      "-I- found 244 emitters\n",
      "Blink too far from emitters: 6263\n",
      "Emitter is out of bound: 28\n",
      "Bad fitting grade: 9\n",
      "Emitters intensity is too low: 5189\n",
      "-I- updated emitters time traces\n",
      "Experimental Data was loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MiniConda\\lib\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "if(chop[1]-chop[0]<2000):\n",
    "    max_size = int((chop[1]-chop[0])/500)\n",
    "else:\n",
    "    max_size = int((chop[1]-chop[0])/1000)\n",
    "    \n",
    "resolution_nm = pixel_length/scale_size #[nm]\n",
    "\n",
    "if(LoadData):\n",
    "    trajectories, clusterCoordinations = [], []\n",
    "    for i, file in enumerate(file_names):\n",
    "        print(\"**** Analyzing Tiff number {} ****\".format(i+1))\n",
    "        # Load TIFF files and create data_set\n",
    "        Data_Set = CreateDataSet(file, chop)\n",
    "        \n",
    "        # Segment the experiment before and after laser activation\n",
    "        seg = segment(Data_Set, threshold=0.15, window_size=100)\n",
    "        \n",
    "        # Filter beads (if True)\n",
    "        if(FilterBeads):\n",
    "            Data_Set = Filter_beads(Data_Set)\n",
    "        \n",
    "        # Background noise cleaning\n",
    "        Data_Set = clean_bg_noise(Data_Set, patch_length=5)\n",
    "        \n",
    "        # Clusters localization\n",
    "        Max_Data_Set = CreateMaxDataSet(Data_Set, max_size, seg)\n",
    "        DataThreshold, MaxThreshold = calc_threshold(Data_Set, Max_Data_Set)\n",
    "        coordinates = LocalizeEmitters(Max_Data_Set, MaxThreshold, qualityThreshold, pixel_length, resolution_nm, merging_radius)\n",
    "        \n",
    "        # Create time traces for each cluster\n",
    "        timeTraces = ExtractTimeTraces(Data_Set[seg:, :, :], coordinates, pixel_length, resolution_nm, qualityThreshold, DataThreshold, merging_radius)\n",
    "        \n",
    "        # Save the time traces and clusters locations of all experiments in a list\n",
    "        trajectories.append(timeTraces)\n",
    "        clusterCoordinations.append(coordinates)\n",
    "        \n",
    "        # The coordinations file would be saved as 'coordinated.npy'\n",
    "    np.save('clusterCoordinations', clusterCoordinations)\n",
    "\n",
    "    # Extract the features that would serve as the neural network's input\n",
    "    X_test = feature_extraction(trajectories, DataThreshold, numOfBins)\n",
    "else:\n",
    "    # Load features of an already analyzed experiment\n",
    "    X_test = LoadFinalDataSet()\n",
    "\n",
    "print(\"Experimental Data was loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-understanding",
   "metadata": {},
   "source": [
    "### Plot Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "tamil-seeking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd478e64b924562948c6fa162c76a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6f67e14e3943ba9e7555c701a59d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(1, numOfBins + 1), X_test[i])\n",
    "    plt.xlabel('Bins')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.xticks(np.arange(1, numOfBins + 1))\n",
    "    plt.title(\"Exp \" + str(i+1) + \" histogram\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-meditation",
   "metadata": {},
   "source": [
    "### Export to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "polish-watts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data export completed successfully\n"
     ]
    }
   ],
   "source": [
    "for i in range(X_test.shape[0]):\n",
    "    np.savetxt(\"Exp_\"+str(i+1)+\"_histogram.csv\", X_test[i], delimiter=',')\n",
    "    np.savetxt(\"Exp_\"+str(i+1)+\"_localization.csv\", clusterCoordinations[i], delimiter=',')\n",
    "\n",
    "print(\"Data export completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-queen",
   "metadata": {},
   "source": [
    "### Visualize Localizations\n",
    "The next block will plot a max projection image of the last experiment with the localization marked on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "great-ferry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf592a93492e4350bf9dfa3c700cf7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if(LoadData):\n",
    "    debug_entire_exp(Max_Data_Set, coordinates, scale_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-harvey",
   "metadata": {},
   "source": [
    "### Simulated Data Setup\n",
    "\n",
    "If you chose to simulate the training data, you would need to specify the following parameters:\n",
    "\n",
    "**numOfClusters** - Specifies the number of simulated clusters in each simulation (relevant only if CreateSimulatedData is set to True).\n",
    "\n",
    "**bleach_proba** - Specifies the bleaching probability of the used fluorophore.\n",
    "\n",
    "**TrainSetSize** - Specifies the number of simulated experiments to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "automated-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfClusters = 200\n",
    "bleach_proba = 0.41\n",
    "TrainSetSize = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-muscle",
   "metadata": {},
   "source": [
    "### Create Simulated Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "affected-premiere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Simulated Data was created successfully\n"
     ]
    }
   ],
   "source": [
    "if(CreateSimulatedData):\n",
    "    [X, y] = CreateSimulatedDataSet(TrainSetSize, numOfClusters, bleach_proba, numOfBins)\n",
    "else:\n",
    "    [X, y] = LoadSimulatedDataSet()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.75)\n",
    "[X_train, X_val, X_test] = Normalization(X_train, X_val, X_test)\n",
    "[X_train, X_val, X_test] = BiasTrick(X_train, X_val, X_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_val = torch.FloatTensor(y_val)\n",
    "\n",
    "print(\"-I- Simulated Data was created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-carpet",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "In the next block we will build the neural network model.\n",
    "\n",
    "**lr** - Specifies the training phase learning rate.\n",
    "\n",
    "**betas** - Specifies the parameters for ADAM optimizer.\n",
    "\n",
    "**batch_size** - Specifies the batch size of the training phase.\n",
    "\n",
    "**epochs** - Specifies the maximal training epoch.\n",
    "\n",
    "**early_stopping** - Specifies the tolerance of the neural network to lack of improvement in the validation loss. For example: early_stopping = 5, would stop the trainig phase if the validation loss did not improve for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "better-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "betas = (0.99, 0.999)\n",
    "batch_size = 4\n",
    "epochs = 1000\n",
    "early_stopping = np.min((int(epochs/5), 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-calgary",
   "metadata": {},
   "source": [
    "### Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "unknown-anniversary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EPOCH 1/1000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MiniConda\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Train loss = 0.032010581344366074\n",
      "Epoch 1 : Validation loss = 0.0034867837093770504\n",
      "--- EPOCH 2/1000 ---\n",
      "Epoch 2 : Train loss = 0.002741472329944372\n",
      "Epoch 2 : Validation loss = 0.0021230459678918123\n",
      "--- EPOCH 3/1000 ---\n",
      "Epoch 3 : Train loss = 0.0019213355844840407\n",
      "Epoch 3 : Validation loss = 0.0016796514391899109\n",
      "--- EPOCH 4/1000 ---\n",
      "Epoch 4 : Train loss = 0.0016077193431556225\n",
      "Epoch 4 : Validation loss = 0.0015210711862891912\n",
      "--- EPOCH 5/1000 ---\n",
      "Epoch 5 : Train loss = 0.0014739680336788297\n",
      "Epoch 5 : Validation loss = 0.0014502544654533267\n",
      "--- EPOCH 6/1000 ---\n",
      "Epoch 6 : Train loss = 0.0014031744794920087\n",
      "Epoch 6 : Validation loss = 0.0014080804539844394\n",
      "--- EPOCH 7/1000 ---\n",
      "Epoch 7 : Train loss = 0.0013563685351982713\n",
      "Epoch 7 : Validation loss = 0.0013794208643957973\n",
      "--- EPOCH 8/1000 ---\n",
      "Epoch 8 : Train loss = 0.0013223503483459353\n",
      "Epoch 8 : Validation loss = 0.001358283800072968\n",
      "--- EPOCH 9/1000 ---\n",
      "Epoch 9 : Train loss = 0.001295644324272871\n",
      "Epoch 9 : Validation loss = 0.0013416152214631438\n",
      "--- EPOCH 10/1000 ---\n",
      "Epoch 10 : Train loss = 0.001273528323508799\n",
      "Epoch 10 : Validation loss = 0.001329330145381391\n",
      "--- EPOCH 11/1000 ---\n",
      "Epoch 11 : Train loss = 0.001254880684427917\n",
      "Epoch 11 : Validation loss = 0.00131938885897398\n",
      "--- EPOCH 12/1000 ---\n",
      "Epoch 12 : Train loss = 0.0012389823095872998\n",
      "Epoch 12 : Validation loss = 0.0013116283807903528\n",
      "--- EPOCH 13/1000 ---\n",
      "Epoch 13 : Train loss = 0.0012250107247382402\n",
      "Epoch 13 : Validation loss = 0.00130518339574337\n",
      "--- EPOCH 14/1000 ---\n",
      "Epoch 14 : Train loss = 0.0012124485801905394\n",
      "Epoch 14 : Validation loss = 0.0013000861508771777\n",
      "--- EPOCH 15/1000 ---\n",
      "Epoch 15 : Train loss = 0.0012009228812530637\n",
      "Epoch 15 : Validation loss = 0.0012958425795659423\n",
      "--- EPOCH 16/1000 ---\n",
      "Epoch 16 : Train loss = 0.0011901822872459888\n",
      "Epoch 16 : Validation loss = 0.001292186207138002\n",
      "--- EPOCH 17/1000 ---\n",
      "Epoch 17 : Train loss = 0.0011803334346041083\n",
      "Epoch 17 : Validation loss = 0.001289265463128686\n",
      "--- EPOCH 18/1000 ---\n",
      "Epoch 18 : Train loss = 0.001171197509393096\n",
      "Epoch 18 : Validation loss = 0.0012871198123320937\n",
      "--- EPOCH 19/1000 ---\n",
      "Epoch 19 : Train loss = 0.0011625849874690175\n",
      "Epoch 19 : Validation loss = 0.0012850473867729306\n",
      "--- EPOCH 20/1000 ---\n",
      "Epoch 20 : Train loss = 0.0011543923756107688\n",
      "Epoch 20 : Validation loss = 0.001283432124182582\n",
      "--- EPOCH 21/1000 ---\n",
      "Epoch 21 : Train loss = 0.001146661932580173\n",
      "Epoch 21 : Validation loss = 0.0012821567943319678\n",
      "--- EPOCH 22/1000 ---\n",
      "Epoch 22 : Train loss = 0.0011392865562811494\n",
      "Epoch 22 : Validation loss = 0.0012813839130103588\n",
      "--- EPOCH 23/1000 ---\n",
      "Epoch 23 : Train loss = 0.0011324277147650719\n",
      "Epoch 23 : Validation loss = 0.0012806134764105082\n",
      "--- EPOCH 24/1000 ---\n",
      "Epoch 24 : Train loss = 0.0011256281286478043\n",
      "Epoch 24 : Validation loss = 0.0012801106786355376\n",
      "--- EPOCH 25/1000 ---\n",
      "Epoch 25 : Train loss = 0.0011191971134394407\n",
      "Epoch 25 : Validation loss = 0.0012795616639778018\n",
      "--- EPOCH 26/1000 ---\n",
      "Epoch 26 : Train loss = 0.001112881232984364\n",
      "Epoch 26 : Validation loss = 0.0012790964683517814\n",
      "--- EPOCH 27/1000 ---\n",
      "Epoch 27 : Train loss = 0.0011068528983741999\n",
      "Epoch 27 : Validation loss = 0.0012789616594091058\n",
      "--- EPOCH 28/1000 ---\n",
      "Epoch 28 : Train loss = 0.0011010071029886603\n",
      "Epoch 28 : Validation loss = 0.001279105432331562\n",
      "--- EPOCH 29/1000 ---\n",
      "Epoch 29 : Train loss = 0.0010953311575576663\n",
      "Epoch 29 : Validation loss = 0.001279237912967801\n",
      "--- EPOCH 30/1000 ---\n",
      "Epoch 30 : Train loss = 0.0010897517204284668\n",
      "Epoch 30 : Validation loss = 0.0012796750525012612\n",
      "--- EPOCH 31/1000 ---\n",
      "Epoch 31 : Train loss = 0.00108437892049551\n",
      "Epoch 31 : Validation loss = 0.001280347816646099\n",
      "--- EPOCH 32/1000 ---\n",
      "Epoch 32 : Train loss = 0.0010790828382596374\n",
      "Epoch 32 : Validation loss = 0.0012808124301955104\n",
      "--- EPOCH 33/1000 ---\n",
      "Epoch 33 : Train loss = 0.001073901541531086\n",
      "Epoch 33 : Validation loss = 0.0012814621441066265\n",
      "--- EPOCH 34/1000 ---\n",
      "Epoch 34 : Train loss = 0.0010687565663829446\n",
      "Epoch 34 : Validation loss = 0.001281955512240529\n",
      "--- EPOCH 35/1000 ---\n",
      "Epoch 35 : Train loss = 0.001063869334757328\n",
      "Epoch 35 : Validation loss = 0.0012825933517888188\n",
      "--- EPOCH 36/1000 ---\n",
      "Epoch 36 : Train loss = 0.0010589855955913663\n",
      "Epoch 36 : Validation loss = 0.001283254474401474\n",
      "--- EPOCH 37/1000 ---\n",
      "Epoch 37 : Train loss = 0.001054217922501266\n",
      "Epoch 37 : Validation loss = 0.001283986959606409\n",
      "--- EPOCH 38/1000 ---\n",
      "Epoch 38 : Train loss = 0.0010496089234948158\n",
      "Epoch 38 : Validation loss = 0.0012846920872107148\n",
      "--- EPOCH 39/1000 ---\n",
      "Epoch 39 : Train loss = 0.0010449906112626195\n",
      "Epoch 39 : Validation loss = 0.0012853207299485803\n",
      "--- EPOCH 40/1000 ---\n",
      "Epoch 40 : Train loss = 0.0010405692737549543\n",
      "Epoch 40 : Validation loss = 0.0012860664865002036\n",
      "--- EPOCH 41/1000 ---\n",
      "Epoch 41 : Train loss = 0.0010360940359532833\n",
      "Epoch 41 : Validation loss = 0.0012866809265688062\n",
      "--- EPOCH 42/1000 ---\n",
      "Epoch 42 : Train loss = 0.0010317282285541296\n",
      "Epoch 42 : Validation loss = 0.0012875397223979235\n",
      "--- EPOCH 43/1000 ---\n",
      "Reached early stopping criterion\n"
     ]
    }
   ],
   "source": [
    "if(TrainNet):\n",
    "    model = CustomNet(torch.numel(X_train[0]), [128, 128, 128, 128])\n",
    "    \n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "    dl_train = CreateDataLoader(X_train, y_train, batch_size=batch_size)\n",
    "    dl_val = CreateDataLoader(X_val, y_val, batch_size=1)\n",
    "\n",
    "    # ================= Train Net ================\n",
    "    trainer = Trainer(model, criterion, optimizer)\n",
    "    trainer.fit(dl_train, dl_val, num_epochs=epochs, early_stopping=early_stopping, print_every=1)\n",
    "    torch.save(trainer.model.state_dict(), 'model_final_gauss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-force",
   "metadata": {},
   "source": [
    "### Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cutting-marker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "model = CustomNet(torch.numel(X_train[0]), [128, 128, 128, 128])\n",
    "model.load_state_dict(torch.load(preTrainedModel))\n",
    "print(\"Pretrained model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-climate",
   "metadata": {},
   "source": [
    "### Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "wooden-disclaimer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Validation MSE: 2.1616650745272636\n",
      "Dimers Percentage Predictions Per Experiment:\n",
      "1:  44.82960104942322\n",
      "2:  33.570170402526855\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = model(X_val)\n",
    "y_test_pred = model(X_test).squeeze()\n",
    "y_test_pred = torch.max(y_test_pred, torch.zeros(y_test_pred.shape))\n",
    "\n",
    "val_acc = torch.mean(torch.abs(y_val_pred.squeeze() - y_val))\n",
    "print(\"Neural Network Validation MSE:\", 100 * val_acc.item())\n",
    "\n",
    "print(\"Printing dimer percentage per experiment:\")\n",
    "if(y_test_pred.shape == torch.Size([])):\n",
    "    print(\"1: \", 100 * y_test_pred.item())\n",
    "else:\n",
    "    for i in range(y_test_pred.shape[0]):\n",
    "        print(str(i+1)+\": \", 100 * y_test_pred[i].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-parking",
   "metadata": {},
   "source": [
    "### Detection Efficiency Correction\n",
    "Please specify the detection efficiency in your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "billion-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_efficiency = 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-decline",
   "metadata": {},
   "source": [
    "### Calculate Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "official-offer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing corrected dimer percentage per experiment:\n",
      "1:  65.79285869963569\n",
      "2:  47.54001045759694\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing corrected dimer percentage per experiment:\")\n",
    "if(y_test_pred.shape == torch.Size([])):\n",
    "    print(\"1: \", 100 * find_actual_dimers_percentage(y_test_pred.item(), detection_efficiency))\n",
    "else:\n",
    "    for i in range(y_test_pred.shape[0]):\n",
    "         print(str(i+1)+\": \", 100 * find_actual_dimers_percentage(y_test_pred[i].item(), detection_efficiency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-honduras",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
