{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impossible-behavior",
   "metadata": {},
   "source": [
    "# QAFKA\n",
    "\n",
    "## Welcome to QAFKA jupyter notebook\n",
    "\n",
    "\n",
    "In order to run QAFKA please follow the instructions in the following blocks.\n",
    "For more information please visit [link to paper](https://www.google.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-landing",
   "metadata": {},
   "source": [
    "### Parameters Initialization and Setup\n",
    "\n",
    "In the next block we will import the relevant packages for QAFKA.\n",
    "Then, we will define some important parameters for our run.\n",
    "\n",
    "**numOfBins** - Specifies the number of bins in the histogram of the number of blinking events (default: 20)\n",
    "\n",
    "**chop** - Specifies which frames of the experiment you would like to analyze. For example: chop = \\[0, 1000\\] will cause QAFKA to analyze the experiment between the first frame and the 1000th frame.\n",
    "\n",
    "**pixel_length** - Specifies the experiment's pixel size \\[nm\\]\n",
    "\n",
    "**scale_size** - Specifies the resolution scaling. For example: scale_size = 3 and pixel_length = 150 \\[nm\\] would results in a reconstrcuted image with grid size of 50 \\[nm\\].\n",
    "\n",
    "**emitters_size** - Specifies the emitters merging radius for detection. If two clusters would be located within this radius they would be considered as a single cluster.\n",
    "\n",
    "**numOfClusters** - Specifies the number of simulated clusters in each simulated experiment\n",
    "\n",
    "**file_names** - Specifies the names of the TIFF files (at least one experiment is required). For example: 'first_exp.tif'.\n",
    "\n",
    "**qualityThreshold** - Specifies the minimal fitting score for the localization block. (default: 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amazing-tragedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiffcapture in d:\\miniconda\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: Pillow>=2.3.1 in d:\\miniconda\\lib\\site-packages (from tiffcapture) (7.2.0)\n",
      "Requirement already satisfied: numpy>=1.8.0 in d:\\miniconda\\lib\\site-packages (from tiffcapture) (1.18.5)\n",
      "Requirement already satisfied: torch in d:\\miniconda\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in d:\\miniconda\\lib\\site-packages (from torch) (1.18.5)\n",
      "Requirement already satisfied: typing-extensions in d:\\miniconda\\lib\\site-packages (from torch) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiffcapture\n",
    "!pip install torch\n",
    "\n",
    "from datasets import *\n",
    "from dataloaders import *\n",
    "from neural_network import *\n",
    "from trainers import *\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "numOfBins = 20\n",
    "chop = [0, 12000]\n",
    "pixel_length = 150 #[nm]\n",
    "scale_size = 3\n",
    "merging_radius = 150 #[nm]\n",
    "file_names = [r'D:\\Project\\data\\CTLA4\\mEos3.2.tif']\n",
    "qualityThreshold = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-judge",
   "metadata": {},
   "source": [
    "### Run Configuration\n",
    "\n",
    "**LoadData** - Determines if we want to load new experimental data (True) or we want to use an already loaded data (False).\n",
    "\n",
    "**FilterBeads** - Determines if an additional beads filtration algorithm is needed for the experimental data.\n",
    "\n",
    "**CreateSimulatedData** - Determines if we want to use the same training data as before (True) or we want to create new training set (False).\n",
    "\n",
    "**TrainNet** - Determines if we want to train the neural network (True) or not (False).\n",
    "\n",
    "**preTrainedModel** - Specifies the pre-trained model to load in case we do not want to train the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "occupational-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoadData = True\n",
    "FilterBeads = False\n",
    "CreateSimulatedData = True\n",
    "TrainNet = True\n",
    "preTrainedModel = 'model_final_gauss'\n",
    "# Add training time of the net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-correlation",
   "metadata": {},
   "source": [
    "### Analysis Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-substitute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Analyzing Tiff number 1 ****\n",
      "-I- Found segmentation in frame: 431\n",
      "-I- Background noise was filtered\n",
      "Emitter is out of bound: 1572\n",
      "Bad fitting grade: 206\n",
      "Emitters intensity is too low: 122\n",
      "-I- found 989 emitters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MiniConda\\lib\\site-packages\\scipy\\optimize\\minpack.py:808: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n"
     ]
    }
   ],
   "source": [
    "max_size = int((chop[1]-chop[0])/1000)\n",
    "resolution_nm = pixel_length/scale_size #[nm]\n",
    "\n",
    "if(LoadData):\n",
    "    trajectories, clusterCoordinations = [], []\n",
    "    for i, file in enumerate(file_names):\n",
    "        print(\"**** Analyzing Tiff number {} ****\".format(i+1))\n",
    "        # Load TIFF files and create data_set\n",
    "        Data_Set = CreateDataSet(file, chop)\n",
    "        \n",
    "        # Segment the experiment before and after laser activation\n",
    "        seg = segment(Data_Set, threshold=0.15, window_size=100)\n",
    "        \n",
    "        # Filter beads (if True)\n",
    "        if(FilterBeads):\n",
    "            Data_Set = Filter_beads(Data_Set)\n",
    "        \n",
    "        # Background noise cleaning\n",
    "        Data_Set = clean_bg_noise(Data_Set, patch_length=5)\n",
    "        \n",
    "        # Clusters localization\n",
    "        Max_Data_Set = CreateMaxDataSet(Data_Set, max_size, seg)\n",
    "        DataThreshold, MaxThreshold = calc_threshold(Data_Set, Max_Data_Set)\n",
    "        coordinates = LocalizeEmitters(Max_Data_Set, MaxThreshold, qualityThreshold, pixel_length, resolution_nm, merging_radius)\n",
    "        \n",
    "        # Create time traces for each cluster\n",
    "        timeTraces = ExtractTimeTraces(Data_Set[seg:, :, :], coordinates, pixel_length, resolution_nm, qualityThreshold, DataThreshold, merging_radius)\n",
    "        \n",
    "        # Save the time traces and clusters locations of all experiments in a list\n",
    "        trajectories.append(timeTraces)\n",
    "        clusterCoordinations.append(coordinates)\n",
    "        \n",
    "        # The coordinations file would be saved as 'coordinated.npy'\n",
    "        np.save('coordinates', coordinates)\n",
    "\n",
    "    # Extract the features that would serve as the neural network's input\n",
    "    X_test = feature_extraction(trajectories, DataThreshold, numOfBins)\n",
    "else:\n",
    "    # Load features of an already analyzed experiment\n",
    "    X_test = LoadFinalDataSet()\n",
    "\n",
    "print(\"-I- Experimental Data was loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-queen",
   "metadata": {},
   "source": [
    "### Visualize Localizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-ferry",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(LoadData):\n",
    "    # This will plot a max projection image of the last experiment with the localization marked on it\n",
    "    debug_entire_exp(Max_Data_Set, coordinates, scale_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-harvey",
   "metadata": {},
   "source": [
    "### Simulated Data Setup\n",
    "\n",
    "If you chose to simulate the training data, you would need to specify the following parameters:\n",
    "\n",
    "**numOfClusters** - Specifies the number of simulated clusters in each simulation (relevant only if CreateSimulatedData is set to True).\n",
    "\n",
    "**bleach_proba** - Specifies the bleaching probability of the used fluorophore.\n",
    "\n",
    "**TrainSetSize** - Specifies the number of simulated experiments to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "automated-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfClusters = 200\n",
    "bleach_proba = 0.41\n",
    "TrainSetSize = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-muscle",
   "metadata": {},
   "source": [
    "### Create Simulated Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affected-premiere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-I- Simulated Data was created successfully\n"
     ]
    }
   ],
   "source": [
    "if(CreateSimulatedData):\n",
    "    [X, y] = CreateSimulatedDataSet(TrainSetSize, numOfClusters, bleach_proba, numOfBins)\n",
    "else:\n",
    "    [X, y] = LoadSimulatedDataSet()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.75)\n",
    "[X_train, X_val, X_test] = Normalization(X_train, X_val, X_test)\n",
    "[X_train, X_val, X_test] = BiasTrick(X_train, X_val, X_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_val = torch.FloatTensor(y_val)\n",
    "\n",
    "print(\"-I- Simulated Data was created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-carpet",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "In the next block we will build the neural network model.\n",
    "\n",
    "**lr** - Specifies the training phase learning rate.\n",
    "\n",
    "**betas** - Specifies the parameters for ADAM optimizer.\n",
    "\n",
    "**batch_size** - Specifies the batch size of the training phase.\n",
    "\n",
    "**epochs** - Specifies the maximal training epoch.\n",
    "\n",
    "**early_stopping** - Specifies the tolerance of the neural network to lack of improvement in the validation loss. For example: early_stopping = 5, would stop the trainig phase if the validation loss did not improve for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "better-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "betas = (0.99, 0.999)\n",
    "batch_size = 4\n",
    "epochs = 1000\n",
    "early_stopping = np.min((int(epochs/5), 15))\n",
    "\n",
    "model = CustomNet(torch.numel(X_train[0]), [128, 128, 128, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-calgary",
   "metadata": {},
   "source": [
    "### Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unknown-anniversary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EPOCH 1/1000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MiniConda\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Train loss = 0.02865607477724552\n",
      "Epoch 1 : Validation loss = 0.0035835967864841223\n",
      "--- EPOCH 2/1000 ---\n",
      "Epoch 2 : Train loss = 0.0025519728660583496\n",
      "Epoch 2 : Validation loss = 0.0021864688023924828\n",
      "--- EPOCH 3/1000 ---\n",
      "Epoch 3 : Train loss = 0.0019239011453464627\n",
      "Epoch 3 : Validation loss = 0.001804760773666203\n",
      "--- EPOCH 4/1000 ---\n",
      "Epoch 4 : Train loss = 0.0016992019955068827\n",
      "Epoch 4 : Validation loss = 0.0016378731233999133\n",
      "--- EPOCH 5/1000 ---\n",
      "Epoch 5 : Train loss = 0.001567807630635798\n",
      "Epoch 5 : Validation loss = 0.0015431827632710338\n",
      "--- EPOCH 6/1000 ---\n",
      "Epoch 6 : Train loss = 0.001481951680034399\n",
      "Epoch 6 : Validation loss = 0.0014840008225291967\n",
      "--- EPOCH 7/1000 ---\n",
      "Epoch 7 : Train loss = 0.0014230094384402037\n",
      "Epoch 7 : Validation loss = 0.0014449077425524592\n",
      "--- EPOCH 8/1000 ---\n",
      "Epoch 8 : Train loss = 0.00138088408857584\n",
      "Epoch 8 : Validation loss = 0.001417119288817048\n",
      "--- EPOCH 9/1000 ---\n",
      "Epoch 9 : Train loss = 0.0013480607885867357\n",
      "Epoch 9 : Validation loss = 0.00139558722730726\n",
      "--- EPOCH 10/1000 ---\n",
      "Epoch 10 : Train loss = 0.0013210132019594312\n",
      "Epoch 10 : Validation loss = 0.0013784392504021525\n",
      "--- EPOCH 11/1000 ---\n",
      "Epoch 11 : Train loss = 0.001298180897720158\n",
      "Epoch 11 : Validation loss = 0.0013648195890709758\n",
      "--- EPOCH 12/1000 ---\n",
      "Epoch 12 : Train loss = 0.001278494019061327\n",
      "Epoch 12 : Validation loss = 0.001353455358184874\n",
      "--- EPOCH 13/1000 ---\n",
      "Epoch 13 : Train loss = 0.0012614908628165722\n",
      "Epoch 13 : Validation loss = 0.0013440253678709269\n",
      "--- EPOCH 14/1000 ---\n",
      "Epoch 14 : Train loss = 0.0012462142622098327\n",
      "Epoch 14 : Validation loss = 0.0013362421886995435\n",
      "--- EPOCH 15/1000 ---\n",
      "Epoch 15 : Train loss = 0.0012325210263952613\n",
      "Epoch 15 : Validation loss = 0.0013296985998749733\n",
      "--- EPOCH 16/1000 ---\n",
      "Epoch 16 : Train loss = 0.0012200494529679418\n",
      "Epoch 16 : Validation loss = 0.001324135111644864\n",
      "--- EPOCH 17/1000 ---\n",
      "Epoch 17 : Train loss = 0.001208506990224123\n",
      "Epoch 17 : Validation loss = 0.0013192483456805348\n",
      "--- EPOCH 18/1000 ---\n",
      "Epoch 18 : Train loss = 0.001197921228595078\n",
      "Epoch 18 : Validation loss = 0.001315379049628973\n",
      "--- EPOCH 19/1000 ---\n",
      "Epoch 19 : Train loss = 0.0011880696984007955\n",
      "Epoch 19 : Validation loss = 0.001311858999542892\n",
      "--- EPOCH 20/1000 ---\n",
      "Epoch 20 : Train loss = 0.0011788317933678627\n",
      "Epoch 20 : Validation loss = 0.0013089218409731984\n",
      "--- EPOCH 21/1000 ---\n",
      "Epoch 21 : Train loss = 0.0011701624607667327\n",
      "Epoch 21 : Validation loss = 0.0013062625657767057\n",
      "--- EPOCH 22/1000 ---\n",
      "Epoch 22 : Train loss = 0.001161959720775485\n",
      "Epoch 22 : Validation loss = 0.0013040673220530152\n",
      "--- EPOCH 23/1000 ---\n",
      "Epoch 23 : Train loss = 0.001154272467829287\n",
      "Epoch 23 : Validation loss = 0.0013022646307945251\n",
      "--- EPOCH 24/1000 ---\n",
      "Epoch 24 : Train loss = 0.001147023169323802\n",
      "Epoch 24 : Validation loss = 0.0013006326043978333\n",
      "--- EPOCH 25/1000 ---\n",
      "Epoch 25 : Train loss = 0.0011400143848732114\n",
      "Epoch 25 : Validation loss = 0.0012991158291697502\n",
      "--- EPOCH 26/1000 ---\n",
      "Epoch 26 : Train loss = 0.0011333533329889178\n",
      "Epoch 26 : Validation loss = 0.0012976978905498981\n",
      "--- EPOCH 27/1000 ---\n",
      "Epoch 27 : Train loss = 0.001127099385485053\n",
      "Epoch 27 : Validation loss = 0.0012967108050361276\n",
      "--- EPOCH 28/1000 ---\n",
      "Epoch 28 : Train loss = 0.0011210511438548565\n",
      "Epoch 28 : Validation loss = 0.0012956965947523713\n",
      "--- EPOCH 29/1000 ---\n",
      "Epoch 29 : Train loss = 0.0011152155930176377\n",
      "Epoch 29 : Validation loss = 0.0012951464159414172\n",
      "--- EPOCH 30/1000 ---\n",
      "Epoch 30 : Train loss = 0.0011095106601715088\n",
      "Epoch 30 : Validation loss = 0.0012944770278409123\n",
      "--- EPOCH 31/1000 ---\n",
      "Epoch 31 : Train loss = 0.001104001305066049\n",
      "Epoch 31 : Validation loss = 0.0012942225439473987\n",
      "--- EPOCH 32/1000 ---\n",
      "Epoch 32 : Train loss = 0.0010985841508954763\n",
      "Epoch 32 : Validation loss = 0.0012937643332406878\n",
      "--- EPOCH 33/1000 ---\n",
      "Epoch 33 : Train loss = 0.001093262224458158\n",
      "Epoch 33 : Validation loss = 0.0012933018151670694\n",
      "--- EPOCH 34/1000 ---\n",
      "Epoch 34 : Train loss = 0.0010881124762818217\n",
      "Epoch 34 : Validation loss = 0.0012928246287629008\n",
      "--- EPOCH 35/1000 ---\n",
      "Epoch 35 : Train loss = 0.001083189039491117\n",
      "Epoch 35 : Validation loss = 0.0012926908675581217\n",
      "--- EPOCH 36/1000 ---\n",
      "Epoch 36 : Train loss = 0.0010783185716718435\n",
      "Epoch 36 : Validation loss = 0.0012925718910992146\n",
      "--- EPOCH 37/1000 ---\n",
      "Epoch 37 : Train loss = 0.001073572551831603\n",
      "Epoch 37 : Validation loss = 0.0012926518684253097\n",
      "--- EPOCH 38/1000 ---\n",
      "Epoch 38 : Train loss = 0.0010689604096114635\n",
      "Epoch 38 : Validation loss = 0.0012927917996421456\n",
      "--- EPOCH 39/1000 ---\n",
      "Epoch 39 : Train loss = 0.0010644136928021908\n",
      "Epoch 39 : Validation loss = 0.0012928856303915381\n",
      "--- EPOCH 40/1000 ---\n",
      "Epoch 40 : Train loss = 0.0010599781526252627\n",
      "Epoch 40 : Validation loss = 0.0012932403478771448\n",
      "--- EPOCH 41/1000 ---\n",
      "Epoch 41 : Train loss = 0.001055578701198101\n",
      "Epoch 41 : Validation loss = 0.0012933959951624274\n",
      "--- EPOCH 42/1000 ---\n",
      "Epoch 42 : Train loss = 0.0010512979933992028\n",
      "Epoch 42 : Validation loss = 0.0012938168365508318\n",
      "--- EPOCH 43/1000 ---\n",
      "Epoch 43 : Train loss = 0.0010470501147210598\n",
      "Epoch 43 : Validation loss = 0.0012941003078594804\n",
      "--- EPOCH 44/1000 ---\n",
      "Epoch 44 : Train loss = 0.0010428180685266852\n",
      "Epoch 44 : Validation loss = 0.0012944299960508943\n",
      "--- EPOCH 45/1000 ---\n",
      "Epoch 45 : Train loss = 0.0010387785732746124\n",
      "Epoch 45 : Validation loss = 0.001294681685976684\n",
      "--- EPOCH 46/1000 ---\n",
      "Epoch 46 : Train loss = 0.0010345897171646357\n",
      "Epoch 46 : Validation loss = 0.0012953276745975018\n",
      "--- EPOCH 47/1000 ---\n",
      "Epoch 47 : Train loss = 0.0010305584874004126\n",
      "Epoch 47 : Validation loss = 0.001295753289014101\n",
      "--- EPOCH 48/1000 ---\n",
      "Epoch 48 : Train loss = 0.0010266099125146866\n",
      "Epoch 48 : Validation loss = 0.0012961775064468384\n",
      "--- EPOCH 49/1000 ---\n",
      "Epoch 49 : Train loss = 0.0010226841550320387\n",
      "Epoch 49 : Validation loss = 0.0012965882197022438\n",
      "--- EPOCH 50/1000 ---\n",
      "Epoch 50 : Train loss = 0.001018871902488172\n",
      "Epoch 50 : Validation loss = 0.0012972673866897821\n",
      "--- EPOCH 51/1000 ---\n",
      "Epoch 51 : Train loss = 0.0010150712914764881\n",
      "Epoch 51 : Validation loss = 0.0012979974271729589\n",
      "--- EPOCH 52/1000 ---\n",
      "Reached early stopping criterion\n"
     ]
    }
   ],
   "source": [
    "if(TrainNet):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "    dl_train = CreateDataLoader(X_train, y_train, batch_size=batch_size)\n",
    "    dl_val = CreateDataLoader(X_val, y_val, batch_size=1)\n",
    "\n",
    "    # ================= Train Net ================\n",
    "    trainer = Trainer(model, criterion, optimizer)\n",
    "    trainer.fit(dl_train, dl_val, num_epochs=epochs, early_stopping=early_stopping, print_every=1)\n",
    "    torch.save(trainer.model.state_dict(), 'model_final_gauss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-force",
   "metadata": {},
   "source": [
    "### Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cutting-marker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if(TrainNet==False):\n",
    "    model.load_state_dict(torch.load(preTrainedModel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-climate",
   "metadata": {},
   "source": [
    "### Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wooden-disclaimer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Validation MSE: 2.0876633003354073\n",
      "Dimers Percentage Predictions Per Experiment:\n",
      "0: 79.21320796012878\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = model(X_val)\n",
    "y_test_pred = model(X_test).squeeze()\n",
    "y_test_pred = torch.max(y_test_pred, torch.zeros(y_test_pred.shape))\n",
    "\n",
    "val_acc = torch.mean(torch.abs(y_val_pred.squeeze() - y_val))\n",
    "print(\"Neural Network Validation MSE:\", 100 * val_acc.item())\n",
    "\n",
    "print(\"Dimers Percentage Predictions Per Experiment:\")\n",
    "if(y_test_pred.shape == torch.Size([])):\n",
    "    print(\"0:\", 100 * y_test_pred.item())\n",
    "else:\n",
    "    for i in range(y_test_pred.shape[0]):\n",
    "        print(\"i:\", 100 * y_test_pred[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-scanning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
