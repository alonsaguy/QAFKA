{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aI5uCuNCMG6N",
        "JVGQEDUeMnE_",
        "XRbMWCGjJ6f6",
        "Qmbn0CsuKBsg",
        "RMRjqS7CKSQr",
        "zCwSwYw5PnPA",
        "m2vHyyrSKccF",
        "gvYMkG1PKhRm"
      ],
      "authorship_tag": "ABX9TyMmAEyLDHG0oWirqp0VvMBH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alonsaguy/QAFKA/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LibxluwQIlXk"
      },
      "source": [
        "# **QAFKA (Quantitative Analysis of Flourescent Kinetics Algorithm)**\r\n",
        "## **Welcome to QAFKA jupyter notebook**\r\n",
        "\r\n",
        "Disclaimer:\r\n",
        "\r\n",
        "This notebook is based on the following paper:\r\n",
        "*italicized text*\r\n",
        "Automated analysis of fluorescence kinetics in single-molecule localization microscopy data reveals protein stoichiometry. [link to paper](https://TBD)\r\n",
        "\r\n",
        "And source code found in: https://github.com/alonsaguy/QAFKA\r\n",
        "\r\n",
        "Please also cite this original paper when using or developing this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDj8-jvVL3lt"
      },
      "source": [
        "## **Initialize Colab Session**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI5uCuNCMG6N"
      },
      "source": [
        "### **Mount your Google Drive**\r\n",
        "---\r\n",
        "To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\r\n",
        "\r\n",
        "Play the cell below to mount your Google Drive and follow the link. In the new browser window, select your drive and select 'Allow', copy the code, paste into the cell and press enter. This will give Colab access to the data on the drive. \r\n",
        "\r\n",
        "Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeiIleBzL2Kq"
      },
      "source": [
        "#@markdown ##Run this cell to connect your Google Drive to Colab\r\n",
        "\r\n",
        "#@markdown * Click on the URL. \r\n",
        "\r\n",
        "#@markdown * Sign in your Google Account. \r\n",
        "\r\n",
        "#@markdown * Copy the authorization code. \r\n",
        "\r\n",
        "#@markdown * Enter the authorization code. \r\n",
        "\r\n",
        "#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\". \r\n",
        "\r\n",
        "#mounts user's Google Drive to Google Colab.\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVGQEDUeMnE_"
      },
      "source": [
        "## **Intall QAFKA and dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOthKFjqMnV-"
      },
      "source": [
        "Notebook_version = ['1.12']\r\n",
        "\r\n",
        "!pip install fpdf\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim\r\n",
        "from torch.utils.data import TensorDataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import numpy as np\r\n",
        "import tiffcapture as tc\r\n",
        "import os\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from scipy.stats import geom, nbinom\r\n",
        "import scipy.optimize as opt\r\n",
        "\r\n",
        "def clean_bg_noise(data, patch_length):\r\n",
        "    [N, H, W] = data.shape\r\n",
        "    clean_data = np.zeros_like(data)\r\n",
        "    half_size = int(np.floor(patch_length/2))\r\n",
        "    for i in range(H):\r\n",
        "        for j in range(W):\r\n",
        "            up = np.min([H - 1, i + half_size])\r\n",
        "            down = np.max([0, i - half_size])\r\n",
        "            right = np.min([W - 1, j + half_size])\r\n",
        "            left = np.max([0, j - half_size])\r\n",
        "            clean_data[:, i, j] = data[:, i, j] - np.mean(data[:, down:up, left:right])\r\n",
        "    print(\"-I- Background noise was filtered\")\r\n",
        "    return clean_data\r\n",
        "\r\n",
        "def segment(data, threshold, window_size):\r\n",
        "    ref_mean = np.mean(data)\r\n",
        "    ref_std = np.std(data)\r\n",
        "\r\n",
        "    for i in range(data.shape[0]-window_size):\r\n",
        "        curr_mean = np.mean(data[i:i+window_size])\r\n",
        "        curr_std = np.std(data[i:i+window_size])\r\n",
        "\r\n",
        "        if((np.abs(curr_mean-ref_mean)/ref_mean + np.abs(curr_std-ref_std)/ref_std) < threshold):\r\n",
        "            print(\"-I- Found segmentation in frame:\", i)\r\n",
        "            return i\r\n",
        "\r\n",
        "    print(\"Error: could not segment, choosing frame 500 as segmentation frame\")\r\n",
        "    return 500\r\n",
        "\r\n",
        "def calc_threshold(raw_data, max_data):\r\n",
        "    \"\"\"\r\n",
        "        Calculating a \"good\" threshold fot peak detection\r\n",
        "        :param raw_data: Tensor [image_size] of one experiment\r\n",
        "        :return: float as the threshold.\r\n",
        "    \"\"\"\r\n",
        "    return 2 * np.abs(np.min(raw_data)) + np.std(raw_data), 1.5 * np.abs(np.mean(np.median(max_data, axis=1), axis=1) + np.std(max_data))\r\n",
        "\r\n",
        "def draw_circle(size, rad):\r\n",
        "    # size should be odd\r\n",
        "    circle = np.zeros([size, size], dtype=int)\r\n",
        "    mid = int((size-1)/2)\r\n",
        "    for i in range(size):\r\n",
        "        for j in range(size):\r\n",
        "            if((i-mid)**2 + (j-mid)**2 <= rad**2):\r\n",
        "                circle[i, j] = 1\r\n",
        "\r\n",
        "    return circle\r\n",
        "\r\n",
        "def gauss2d(xy, offset, amp, x0, y0, sigma):\r\n",
        "    # Fit patch to gaussian\r\n",
        "    x, y = xy\r\n",
        "    return offset + amp * np.exp(-((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\r\n",
        "\r\n",
        "def Normalization(X_train, X_val, X_test):\r\n",
        "    \"\"\"\r\n",
        "        Normalizing the data by the statistics of X_train\r\n",
        "        :return: normalized X_train, X_val, X_test.\r\n",
        "    \"\"\"\r\n",
        "    for i in range(X_train.shape[0]):\r\n",
        "        X_train[i, :] /= np.sum(X_train[i])\r\n",
        "    for i in range(X_val.shape[0]):\r\n",
        "        X_val[i, :] /= np.sum(X_val[i])\r\n",
        "    for i in range(X_test.shape[0]):\r\n",
        "        X_test[i, :] /= np.sum(X_test[i])\r\n",
        "\r\n",
        "    mean = np.mean(X_train, axis=0)\r\n",
        "    std = np.std(X_train, axis=0)\r\n",
        "    std[np.where(std == 0)] = 1\r\n",
        "    X_train = (X_train - mean) / std\r\n",
        "    X_val = (X_val - mean) / std\r\n",
        "    X_test = (X_test - mean) / std\r\n",
        "\r\n",
        "    return torch.FloatTensor(X_train), torch.FloatTensor(X_val), torch.FloatTensor(X_test)\r\n",
        "\r\n",
        "def fit(n_blinks_hist):\r\n",
        "    \"\"\"\r\n",
        "        Calculates the MLE for the percentage of dimers in an experiment\r\n",
        "        :param n_blinks_hist: Tensor [numOfBIns] for the n_blinks histogram in an exp.\r\n",
        "        :param p: Float for the bleaching probability of a cluster\r\n",
        "        :return: alpha - the dimers percentage in the experiment.\r\n",
        "    \"\"\"\r\n",
        "    best_alpha, best_p = -1, -1\r\n",
        "    max_val = -np.inf\r\n",
        "    for j in range(200, 500):\r\n",
        "        p = j / 1000\r\n",
        "        for i in range(101):\r\n",
        "            alpha = i / 100\r\n",
        "            val = 0\r\n",
        "            for bin in range(1, len(n_blinks_hist) + 1):\r\n",
        "                a = p * (1 - p) ** (bin-1)\r\n",
        "                b = (bin * (p ** 2)) * (1 - p) ** (bin-1)\r\n",
        "                val += n_blinks_hist[bin - 1] * np.log((1 - alpha) * a + alpha * b)\r\n",
        "\r\n",
        "            if(val > max_val):\r\n",
        "                max_val = val\r\n",
        "                best_alpha = alpha\r\n",
        "                best_p = p\r\n",
        "\r\n",
        "    return best_alpha, best_p\r\n",
        "\r\n",
        "def plot_n_blinks(n_blinks, bleach_proba):\r\n",
        "    for i in range(n_blinks.shape[0]):\r\n",
        "        plt.plot(n_blinks[i]/np.sum(n_blinks[i]), label='{}'.format(i))\r\n",
        "    x1 = np.arange(geom.ppf(0.0001, bleach_proba), geom.ppf(0.9999, bleach_proba))\r\n",
        "    x2 = np.arange(nbinom.ppf(0.0001, 2, bleach_proba), nbinom.ppf(0.9999, 2, bleach_proba))\r\n",
        "    plt.plot(x1 - 1, geom.pmf(x1, bleach_proba), label='Monomers only')\r\n",
        "    plt.plot(x2, nbinom.pmf(x2, 2, bleach_proba), label='Dimers only')\r\n",
        "    plt.legend()\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "def estimate_by_average(n_blinks_hist, bleach_proba):\r\n",
        "    total_blinks = np.dot(n_blinks_hist, np.arange(1, 21))\r\n",
        "    total_clusters = np.sum(n_blinks_hist)\r\n",
        "    best_alpha = None\r\n",
        "    best_diff = np.inf\r\n",
        "    for i in range(101):\r\n",
        "        alpha = i/100\r\n",
        "        # Monomers part * Average monomers blinks + Dimers part * Average dimers blinks\r\n",
        "        approx = ((1-alpha)*total_clusters)*((1/bleach_proba)) + \\\r\n",
        "                 (alpha*total_clusters)*((2*(1-bleach_proba)/bleach_proba)+1)\r\n",
        "        diff = np.abs(approx - total_blinks)\r\n",
        "        if(diff < best_diff):\r\n",
        "            best_diff = diff\r\n",
        "            best_alpha = alpha\r\n",
        "\r\n",
        "    return best_alpha\r\n",
        "\r\n",
        "def kmeans(data):\r\n",
        "    from sklearn.manifold import TSNE\r\n",
        "    from sklearn.cluster import KMeans\r\n",
        "    X_embedded = TSNE(n_components=2).fit_transform(data)\r\n",
        "    nbrs = KMeans(n_clusters=2).fit(data)\r\n",
        "    pred = nbrs.predict(data)\r\n",
        "    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=pred)\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "def plotting(hist, p):\r\n",
        "    hist /= np.sum(hist)\r\n",
        "    x1 = np.arange(geom.ppf(0.0001, p), geom.ppf(0.9999, p))\r\n",
        "    n = 2\r\n",
        "    x2 = np.arange(nbinom.ppf(0.001, n, p), nbinom.ppf(0.999, n, p))\r\n",
        "    plt.plot(hist, label='Experiment histogram')\r\n",
        "    plt.plot(x2 + 1, nbinom.pmf(x2, n, p), label='Dimers histogram')\r\n",
        "    plt.plot(x1-1, geom.pmf(x1, p), label='Monomers histogram')\r\n",
        "    plt.xticks(np.arange(20), np.arange(1, 21))\r\n",
        "    plt.xlabel('Number of blinks')\r\n",
        "    plt.ylabel('Probability')\r\n",
        "    plt.title('Monomers and dimers Nblinks model')\r\n",
        "    plt.legend()\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "def CoordiantionsComparison(filename, myCoordination):\r\n",
        "    data = np.loadtxt(filename, delimiter=' ')\r\n",
        "    col = data[:, 0] / 40200\r\n",
        "    row = data[:, 1] / 40270\r\n",
        "    plt.scatter(row, col)\r\n",
        "    plt.scatter(myCoordination[:, 0]/251, myCoordination[:, 1]/257)\r\n",
        "    plt.xlabel('x [normalized units]')\r\n",
        "    plt.ylabel('y [normalized units]')\r\n",
        "    plt.legend(['Tims localizations', 'My localizations'])\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "def debug_helper(data, emitters_grid, fit, obs, center_x, center_y, fit_quality, patch_length):\r\n",
        "    plt.gca().invert_yaxis()\r\n",
        "    plt.subplot(221)\r\n",
        "    plt.imshow(data)\r\n",
        "    plt.plot(center_y, center_x, color='r', marker='x')\r\n",
        "    plt.subplot(222)\r\n",
        "    plt.imshow(emitters_grid)\r\n",
        "    plt.subplot(223)\r\n",
        "    plt.title(\"Quality {}\".format(fit_quality))\r\n",
        "    plt.imshow(fit.reshape([patch_length, patch_length]))\r\n",
        "    plt.subplot(224)\r\n",
        "    plt.title(\"Localization {}, {}\".format(center_x, center_y))\r\n",
        "    plt.imshow(obs.reshape([patch_length, patch_length]))\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "def MLE(n_blinks_hist, p, d):\r\n",
        "    \"\"\"\r\n",
        "        Calculates the MLE for the percentage of dimers in an experiment\r\n",
        "        :param n_blinks_hist: Tensor [numOfBIns] for the n_blinks histogram in an exp.\r\n",
        "        :param p: Float for the bleaching probability of a cluster\r\n",
        "        :return: alpha - the dimers percentage in the experiment.\r\n",
        "    \"\"\"\r\n",
        "    best_alpha = -1\r\n",
        "    max_val = -np.inf\r\n",
        "\r\n",
        "    val_vec = np.zeros(100)\r\n",
        "    hist_vec = np.ones(int(np.sum(n_blinks_hist)))\r\n",
        "    for i in range(len(n_blinks_hist) - 1):\r\n",
        "        hist_vec[int(np.sum(n_blinks_hist[:i])):int(np.sum(n_blinks_hist[:i+1]))] = i + 1\r\n",
        "\r\n",
        "    for i in range(101):\r\n",
        "        alpha = i / 100\r\n",
        "        val = 0\r\n",
        "        for n in hist_vec:\r\n",
        "            if(n > 1):\r\n",
        "                a = p * (1 - p) ** (n - 1)\r\n",
        "                b = (d * ((n - 1) * (p ** 2)) * (1 - p) ** (n - 2) + (1-d) * p * (1 - p) ** (n-1))\r\n",
        "            else:\r\n",
        "                a = p * (1 - p) ** (n - 1)\r\n",
        "                b = (1-d) * p * (1 - p) ** (n-1)\r\n",
        "\r\n",
        "            val += np.log((1 - alpha) * a + alpha * b)\r\n",
        "\r\n",
        "        if(val > max_val):\r\n",
        "            max_val = val\r\n",
        "            best_alpha = alpha\r\n",
        "        val_vec[i-1] = val\r\n",
        "    plt.plot(val_vec)\r\n",
        "    plt.title(\"Real dimers percentage: {}\".format(best_alpha))\r\n",
        "    plt.show()\r\n",
        "    return best_alpha\r\n",
        "\r\n",
        "def find_atual_dimers_percentage(m, d):\r\n",
        "    real_percentage = 1/(((1-m)/m + 2) * d - 1)\r\n",
        "    return real_percentage\r\n",
        "\r\n",
        "def debug_entire_exp(Max_Data_Set, coordinates):\r\n",
        "    new_img = np.zeros([5 * Max_Data_Set[1].shape[0], 5 * Max_Data_Set[1].shape[1]])\r\n",
        "    for i in range(Max_Data_Set[1].shape[0] - 1):\r\n",
        "        for j in range(Max_Data_Set[1].shape[1] - 1):\r\n",
        "            new_img[(5 * i):(5 * (i + 1)), (5 * j):(5 * (j + 1))] = np.max(Max_Data_Set[:, i, j])\r\n",
        "    plt.imshow(new_img)\r\n",
        "    plt.scatter(coordinates[:, 1], coordinates[:, 0], color='r', marker='x')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "class Trainer():\r\n",
        "    \"\"\"\r\n",
        "    A class abstracting the various tasks of training models.\r\n",
        "    Provides methods at multiple levels of granularity:\r\n",
        "    - Multiple epochs (fit)\r\n",
        "    - Single epoch (train_epoch/test_epoch)\r\n",
        "    - Single batch (train_batch/test_batch)\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, model, loss_fn, optimizer, device='cpu'):\r\n",
        "        \"\"\"\r\n",
        "        Initialize the trainer.\r\n",
        "        :param model: Instance of the model to train.\r\n",
        "        :param loss_fn: The loss function to evaluate with.\r\n",
        "        :param optimizer: The optimizer to train with.\r\n",
        "        :param device: torch.device to run training on (CPU or GPU).\r\n",
        "        \"\"\"\r\n",
        "        self.model = model\r\n",
        "        self.loss_fn = loss_fn\r\n",
        "        self.optimizer = optimizer\r\n",
        "        self.device = device\r\n",
        "        model.to(self.device)\r\n",
        "\r\n",
        "    def fit(self, dl_train: DataLoader, dl_test: DataLoader,\r\n",
        "            num_epochs, early_stopping=50, print_every=1, **kw):\r\n",
        "        \"\"\"\r\n",
        "        Trains the model for multiple epochs with a given training set,\r\n",
        "        and calculates validation loss over a given validation set.\r\n",
        "        :param dl_train: Dataloader for the training set.\r\n",
        "        :param dl_test: Dataloader for the test set.\r\n",
        "        :param num_epochs: Number of epochs to train for.\r\n",
        "        :param early_stopping: Whether to stop training early if there is no\r\n",
        "            test loss improvement for this number of epochs.\r\n",
        "        :param print_every: Print progress every this number of epochs.\r\n",
        "        :return: Train and test losses per epoch.\r\n",
        "        \"\"\"\r\n",
        "        train_loss, val_loss = [], []\r\n",
        "        best_loss = None\r\n",
        "        best_model = None\r\n",
        "        epochs_without_improvement = 0\r\n",
        "\r\n",
        "        for epoch in range(num_epochs):\r\n",
        "            print(f'--- EPOCH {epoch + 1}/{num_epochs} ---')\r\n",
        "\r\n",
        "            loss = self.train_epoch(dl_train, **kw)\r\n",
        "            train_loss.append(loss)\r\n",
        "\r\n",
        "            loss = self.test_epoch(dl_test, **kw)\r\n",
        "            val_loss.append(loss)\r\n",
        "\r\n",
        "            if(epoch == 0):\r\n",
        "                best_loss = loss\r\n",
        "            else:\r\n",
        "                if(loss >= best_loss):\r\n",
        "                    epochs_without_improvement += 1\r\n",
        "                    if(epochs_without_improvement > early_stopping):\r\n",
        "                        print(\"Reached early stopping criterion\")\r\n",
        "                        self.model.load_state_dict(torch.load('best_model'))\r\n",
        "                        break\r\n",
        "                else:\r\n",
        "                    epochs_without_improvement = 0\r\n",
        "                    best_loss = loss\r\n",
        "                    torch.save(self.model.state_dict(), 'best_model')\r\n",
        "\r\n",
        "            if epoch % print_every == 0 or epoch == num_epochs - 1:\r\n",
        "                print(\"Epoch\", epoch + 1, \": Train loss =\", train_loss[-1].item())\r\n",
        "                print(\"Epoch\", epoch + 1, \": Validation loss =\", val_loss[-1].item())\r\n",
        "\r\n",
        "\r\n",
        "    def train_epoch(self, dl_train: DataLoader, **kw):\r\n",
        "        \"\"\"\r\n",
        "        Train once over a training set (single epoch).\r\n",
        "        :param dl_train: DataLoader for the training set.\r\n",
        "        :param kw: Keyword args supported by _foreach_batch.\r\n",
        "        :return: An EpochResult for the epoch.\r\n",
        "        \"\"\"\r\n",
        "        self.model.train()\r\n",
        "        total_loss = 0\r\n",
        "        cnt = 0\r\n",
        "        for X_train, y_train in dl_train:\r\n",
        "            self.optimizer.zero_grad()\r\n",
        "\r\n",
        "            # Forward pass\r\n",
        "            y_pred = self.model(X_train)\r\n",
        "\r\n",
        "            # Compute Loss\r\n",
        "            loss = self.loss_fn(y_pred.squeeze(), y_train)\r\n",
        "            total_loss += loss\r\n",
        "\r\n",
        "            # Backward pass\r\n",
        "            loss.backward()\r\n",
        "            self.optimizer.step()\r\n",
        "\r\n",
        "            cnt += 1\r\n",
        "        return total_loss / cnt\r\n",
        "\r\n",
        "    def test_epoch(self, dl_test: DataLoader, **kw):\r\n",
        "        \"\"\"\r\n",
        "        Evaluate model once over a test set (single epoch).\r\n",
        "        :param dl_test: DataLoader for the test set.\r\n",
        "        :param kw: Keyword args supported by _foreach_batch.\r\n",
        "        :return: An EpochResult for the epoch.\r\n",
        "        \"\"\"\r\n",
        "        self.model.eval()\r\n",
        "        total_loss = 0\r\n",
        "        cnt = 0\r\n",
        "        for X_test, y_test in dl_test:\r\n",
        "            # Forward pass\r\n",
        "            y_pred = self.model(X_test)\r\n",
        "            total_loss += self.loss_fn(y_pred.squeeze(), y_test)\r\n",
        "            cnt += 1\r\n",
        "\r\n",
        "        return total_loss / cnt\r\n",
        "\r\n",
        "class ONE_LAYER_NET(torch.nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size):\r\n",
        "        super(ONE_LAYER_NET, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.hidden_size = [self.input_size] + hidden_size\r\n",
        "\r\n",
        "        modules = []\r\n",
        "        modules.append(torch.nn.Linear(self.hidden_size[0], 1, bias=True))\r\n",
        "        self.net = nn.Sequential(*modules)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        output = self.net(x)\r\n",
        "        return output\r\n",
        "\r\n",
        "class SIMPLE_FC_NET(torch.nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size):\r\n",
        "        super(SIMPLE_FC_NET, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.hidden_size = [self.input_size] + hidden_size\r\n",
        "\r\n",
        "        modules = []\r\n",
        "\r\n",
        "        for i in range(len(self.hidden_size) - 1):\r\n",
        "            modules.append(torch.nn.Linear(self.hidden_size[i], self.hidden_size[i+1], bias=True))\r\n",
        "            modules.append(torch.nn.PReLU())\r\n",
        "\r\n",
        "        modules.append(torch.nn.Linear(self.hidden_size[-1], 1))\r\n",
        "\r\n",
        "        self.net = nn.Sequential(*modules)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        output = self.net(x)\r\n",
        "        return output\r\n",
        "\r\n",
        "class CustomNet(torch.nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size):\r\n",
        "        super(CustomNet, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.hidden_size = [self.input_size] + hidden_size\r\n",
        "\r\n",
        "        modules = []\r\n",
        "\r\n",
        "        for i in range(len(self.hidden_size) - 1):\r\n",
        "            modules.append(torch.nn.Linear(self.hidden_size[i], self.hidden_size[i+1]))\r\n",
        "            modules.append(torch.nn.PReLU())\r\n",
        "\r\n",
        "        modules.append(torch.nn.Linear(self.hidden_size[-1], 1))\r\n",
        "\r\n",
        "        self.net = nn.Sequential(*modules)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        output = self.net(x)\r\n",
        "        return output\r\n",
        "\r\n",
        "def BiasTrick(X_train, X_val, X_test):\r\n",
        "    coef = 0.05\r\n",
        "    ones = torch.ones((X_train.shape[0], 1)).type(torch.FloatTensor) * coef\r\n",
        "    X_train = torch.cat((ones, X_train.type(torch.FloatTensor)), dim=1)\r\n",
        "    ones = torch.ones((X_val.shape[0], 1)).type(torch.FloatTensor) * coef\r\n",
        "    X_val = torch.cat((ones, X_val.type(torch.FloatTensor)), dim=1)\r\n",
        "    ones = torch.ones((X_test.shape[0], 1)).type(torch.FloatTensor) * coef\r\n",
        "    X_test = torch.cat((ones, X_test.type(torch.FloatTensor)), dim=1)\r\n",
        "    return X_train, X_val, X_test\r\n",
        "\r\n",
        "def CreateMaxDataSet(data, max_size, seg):\r\n",
        "    # Divide the Data_Set into sum_size datasets summing every frame of data in each segment\r\n",
        "    max_data = np.zeros([max_size, data.shape[1], data.shape[2]])\r\n",
        "    divider = int(data.shape[0]/max_size)\r\n",
        "    n = divider - seg\r\n",
        "    max_data[0, :, :] = np.max(data[:n, :, :], axis=0)\r\n",
        "    n = divider\r\n",
        "    for i in range(1, max_size):\r\n",
        "        max_data[i, :, :] = np.max(data[((i-1)*n):(i*n), :, :], axis=0)\r\n",
        "    return max_data\r\n",
        "\r\n",
        "def LocalizeEmitters(data, sum_threshold, quality_threshold, pixel_length, resolution_nm, emitters_size):\r\n",
        "    total_frames, max_row, max_col = data.shape\r\n",
        "    emitters_grid = np.zeros((int(max_row * pixel_length / resolution_nm),\r\n",
        "                              int(max_col * pixel_length / resolution_nm)), dtype=int)\r\n",
        "    emitters_coordinates = []\r\n",
        "    emitters_cnt = 0\r\n",
        "\r\n",
        "    patch_length = 9  # Determine the patch we are going to fit to be of size [patch_length x patch length]\r\n",
        "    xy = np.zeros([2, int(patch_length ** 2)])\r\n",
        "    for i1 in range(patch_length):\r\n",
        "        for j1 in range(patch_length):\r\n",
        "            xy[:, int(i1 * patch_length + j1)] = [i1, j1]\r\n",
        "\r\n",
        "    # Create the shape of a circle for the emitters grid\r\n",
        "    circle = draw_circle(size=int(patch_length * pixel_length / resolution_nm),\r\n",
        "                         rad=int(emitters_size / resolution_nm))\r\n",
        "\r\n",
        "    low_intensity, bad_fit, out_of_bound = 0, 0, 0\r\n",
        "    for frame in range(total_frames):\r\n",
        "        img_arr = data[frame, :, :]\r\n",
        "        threshold = sum_threshold[frame]\r\n",
        "        if (np.max(img_arr) > threshold):\r\n",
        "            potential_peaks = np.where(img_arr > threshold)\r\n",
        "            for i in range(len(potential_peaks[0])):\r\n",
        "                row, col = potential_peaks[0][i], potential_peaks[1][i]\r\n",
        "\r\n",
        "                # Handle clusters in case of exceeding image shape\r\n",
        "                up = int(row + np.floor(patch_length / 2)) + 1\r\n",
        "                down = int(row - np.floor(patch_length / 2))\r\n",
        "                left = int(col - np.floor(patch_length / 2))\r\n",
        "                right = int(col + np.floor(patch_length / 2)) + 1\r\n",
        "\r\n",
        "                # Ignore out of bound blinks\r\n",
        "                if up > max_row or down < 0 or left < 0 or right > max_col:\r\n",
        "                    out_of_bound += 1\r\n",
        "                    continue\r\n",
        "\r\n",
        "                # Initial guess\r\n",
        "                x0, y0 = int(np.floor(patch_length / 2)), int(np.floor(patch_length / 2))\r\n",
        "\r\n",
        "                # Fit the patch to a gaussian\r\n",
        "                zobs = (img_arr[down:up, left:right]).reshape(1, -1).squeeze()\r\n",
        "\r\n",
        "                # Check if localization is in local maximum\r\n",
        "                if ([down + int(np.argmax(zobs)/patch_length), left + np.argmax(zobs) % patch_length] != [row, col]):\r\n",
        "                    continue\r\n",
        "\r\n",
        "                guess = [np.median(img_arr), np.max(img_arr) - np.min(img_arr), x0, y0, 1]\r\n",
        "                try:\r\n",
        "                    pred_params, uncert_cov = opt.curve_fit(gauss2d, xy, zobs, p0=guess)\r\n",
        "                except:\r\n",
        "                    continue\r\n",
        "\r\n",
        "                fit = gauss2d(xy, *pred_params)\r\n",
        "\r\n",
        "                # If the peak is higher than threshold proceed\r\n",
        "                curr_row = int(np.round(down + pred_params[3]))\r\n",
        "                curr_col = int(np.round(left + pred_params[2]))\r\n",
        "                if (curr_col >= max_col or curr_col < 0 or curr_row >= max_row or curr_row < 0):\r\n",
        "                    out_of_bound += 1\r\n",
        "                    continue\r\n",
        "\r\n",
        "                # Handle same coordinate repetition\r\n",
        "                y = down + pred_params[3]\r\n",
        "                x = left + pred_params[2]\r\n",
        "                center_y = int(np.round(y * pixel_length / resolution_nm))\r\n",
        "                center_x = int(np.round(x * pixel_length / resolution_nm))\r\n",
        "\r\n",
        "                if (img_arr[curr_row, curr_col] > threshold):\r\n",
        "                    # Calculate RMS\r\n",
        "                    zobs /= np.max(zobs)\r\n",
        "                    fit /= np.max(fit)\r\n",
        "                    fit_quality = 1 - np.sqrt(np.mean((zobs - fit) ** 2))\r\n",
        "\r\n",
        "                    # If the fit quality is higher than Value > take the mean value as a new cluster's coordinates\r\n",
        "                    # Ignore fitted gaussian with sigma higher than 1 or lower than 0.3\r\n",
        "                    #if (fit_quality > quality_threshold and pred_params[4] < 1 and pred_params[4] > 0.3):\r\n",
        "                    if (fit_quality > quality_threshold):\r\n",
        "                        # If the current pixel in the grid is already tagged for one of the emitters\r\n",
        "                        if (emitters_grid[center_y, center_x] > 0):\r\n",
        "                            continue\r\n",
        "\r\n",
        "                        mid = int((patch_length * pixel_length / resolution_nm)/2)\r\n",
        "                        # If the emitter is located outside the image boundaries ignore\r\n",
        "                        if(center_y - mid < 0 or center_y + mid + 1 >= emitters_grid.shape[0] or\r\n",
        "                                center_x - mid < 0 or center_x + mid + 1 >= emitters_grid.shape[1]):\r\n",
        "                            continue\r\n",
        "\r\n",
        "                        # Add localization\r\n",
        "                        emitters_cnt += 1\r\n",
        "                        # Update emitters_grid\r\n",
        "                        emitters_grid[(center_y-mid):(center_y+mid+1), (center_x-mid):(center_x+mid+1)] += emitters_cnt * circle\r\n",
        "                        # Update emitters list\r\n",
        "                        emitters_coordinates.append([center_y, center_x])\r\n",
        "                    else:\r\n",
        "                        bad_fit += 1\r\n",
        "                else:\r\n",
        "                    low_intensity += 1\r\n",
        "\r\n",
        "    print(\"Emitter is out of bound:\", out_of_bound)\r\n",
        "    print(\"Bad fitting grade:\", bad_fit)\r\n",
        "    print(\"Emitters intensity is too low:\", low_intensity)\r\n",
        "    print(\"-I- found\", emitters_cnt, \"emitters\")\r\n",
        "\r\n",
        "    return np.array(emitters_coordinates)\r\n",
        "\r\n",
        "def ExtractTimeTraces(raw_data, emitters_coord, pixel_length, resolution_nm, quality_threshold, threshold, emitters_size):\r\n",
        "    total_frames, max_row, max_col = raw_data.shape\r\n",
        "    emitters = np.zeros([emitters_coord.shape[0], total_frames])\r\n",
        "    emitters_size_pix = emitters_size / resolution_nm\r\n",
        "    patch_length = 5 # Determine the patch we are going to fit to be of size [patch_length x patch length]\r\n",
        "    xy = np.zeros([2, int(patch_length ** 2)])\r\n",
        "    for i1 in range(patch_length):\r\n",
        "        for j1 in range(patch_length):\r\n",
        "            xy[:, int(i1 * patch_length + j1)] = [i1, j1]\r\n",
        "\r\n",
        "    low_intensity, bad_fit, out_of_bound, far_from_emitters = 0, 0, 0, 0\r\n",
        "    for frame in range(total_frames):\r\n",
        "        img_arr = raw_data[frame, :, :]\r\n",
        "        if (np.max(img_arr) > threshold):\r\n",
        "            potential_peaks = np.where(img_arr > threshold)\r\n",
        "            for i in range(len(potential_peaks[0])):\r\n",
        "                row, col = potential_peaks[0][i], potential_peaks[1][i]\r\n",
        "\r\n",
        "                # Handle clusters in case of exceeding image shape\r\n",
        "                up = int(row + np.floor(patch_length / 2)) + 1\r\n",
        "                down = int(row - np.floor(patch_length / 2))\r\n",
        "                left = int(col - np.floor(patch_length / 2))\r\n",
        "                right = int(col + np.floor(patch_length / 2)) + 1\r\n",
        "\r\n",
        "                # Ignore out of bound blinks\r\n",
        "                if up > max_row or down < 0 or left < 0 or right > max_col:\r\n",
        "                    out_of_bound += 1\r\n",
        "                    continue\r\n",
        "\r\n",
        "                # Initial guess\r\n",
        "                x0, y0 = int(np.floor(patch_length / 2)), int(np.floor(patch_length / 2))\r\n",
        "\r\n",
        "                zobs = (img_arr[down:up, left:right]).reshape(1, -1).squeeze()\r\n",
        "                guess = [np.median(img_arr), np.max(img_arr) - np.min(img_arr), x0, y0, 1]\r\n",
        "                try:\r\n",
        "                    pred_params, uncert_cov = opt.curve_fit(gauss2d, xy, zobs, p0=guess)\r\n",
        "                except:\r\n",
        "                    continue\r\n",
        "\r\n",
        "                fit = gauss2d(xy, *pred_params)\r\n",
        "\r\n",
        "                # Check if localization is in local maximum\r\n",
        "                if (np.argmax(zobs) != int(np.round(pred_params[2])) * patch_length + int(np.round(pred_params[3]))):\r\n",
        "                    continue\r\n",
        "\r\n",
        "                # If the peak is higher than threshold proceed\r\n",
        "                curr_row = int(np.round(down + pred_params[3]))\r\n",
        "                curr_col = int(np.round(left + pred_params[2]))\r\n",
        "                if (curr_col >= max_col or curr_col < 0 or curr_row >= max_row or curr_row < 0):\r\n",
        "                    out_of_bound += 1\r\n",
        "                    continue\r\n",
        "\r\n",
        "                # Handle same coordinate repetition\r\n",
        "                x = left + pred_params[2]\r\n",
        "                y = down + pred_params[3]\r\n",
        "                center_y = int(np.round(y * (pixel_length / resolution_nm)))\r\n",
        "                center_x = int(np.round(x * (pixel_length / resolution_nm)))\r\n",
        "\r\n",
        "                if (img_arr[curr_row, curr_col] > threshold):\r\n",
        "                    # Calculate RMS\r\n",
        "                    zobs /= np.max(zobs)\r\n",
        "                    fit /= np.max(fit)\r\n",
        "                    fit_quality = 1 - np.sqrt(np.mean((zobs - fit) ** 2))\r\n",
        "\r\n",
        "                    # If the fit quality is higher than Value > take the mean value as a new cluster's coordinates\r\n",
        "                    if (fit_quality > quality_threshold):\r\n",
        "                        min_dist = np.inf\r\n",
        "                        cluster_ind = -1\r\n",
        "                        for k in range(emitters_coord.shape[0]):\r\n",
        "                            dist = np.sqrt((center_y - emitters_coord[k, 0])**2 + (center_x - emitters_coord[k, 1])**2)\r\n",
        "                            if (dist > emitters_size_pix):\r\n",
        "                                continue\r\n",
        "                            if (dist < min_dist):\r\n",
        "                                min_dist = dist\r\n",
        "                                cluster_ind = k\r\n",
        "                        if (min_dist > emitters_size_pix):\r\n",
        "                            far_from_emitters += 1\r\n",
        "                            continue\r\n",
        "                        else:\r\n",
        "                            # Update emitters list with new time trace\r\n",
        "                            emitters[cluster_ind, frame] = pred_params[0] + pred_params[1]  # Offset + Amp of gaussian\r\n",
        "                    else:\r\n",
        "                        bad_fit += 1\r\n",
        "                else:\r\n",
        "                    low_intensity += 1\r\n",
        "\r\n",
        "    print(\"Blink too far from emitters:\", far_from_emitters)\r\n",
        "    print(\"Emitter is out of bound:\", out_of_bound)\r\n",
        "    print(\"Bad fitting grade:\", bad_fit)\r\n",
        "    print(\"Emitters intensity is too low:\", low_intensity)\r\n",
        "    print(\"-I- updated emitters time traces\")\r\n",
        "\r\n",
        "    return emitters\r\n",
        "\r\n",
        "def CreateDataSet(file, chop):\r\n",
        "    \"\"\"\r\n",
        "        Creates trajectories data set out of experimental data\r\n",
        "        :param path: String for the path to the data library\r\n",
        "        :param path: List for start frame and stop frame of the TIFF videos\r\n",
        "        :return: data set [# of trajectories, trajectories length].\r\n",
        "    \"\"\"\r\n",
        "    path = 'D:\\Project\\data'\r\n",
        "    tiff = tc.opentiff(os.path.join(path, file))\r\n",
        "    x0 = tiff.find_and_read(chop[0])\r\n",
        "\r\n",
        "    data = np.zeros((chop[1] - chop[0], x0.shape[0], x0.shape[1]))\r\n",
        "    for i in range(chop[1] - chop[0]):\r\n",
        "        data[i, :, :] = tiff.find_and_read(chop[0] + i)\r\n",
        "\r\n",
        "    return data\r\n",
        "\r\n",
        "def find_trajectories_gauss(raw_data, threshold, quality_threshold):\r\n",
        "    \"\"\"\r\n",
        "        Finds trajectories from the given TIFF video\r\n",
        "        :param raw_data: Tensor [# of trajectories, trajectories length]\r\n",
        "        :param threshold: Float for the minimal threshold for peak detection\r\n",
        "        :return: List of all the different trajectories in time.\r\n",
        "    \"\"\"\r\n",
        "    total_frames, max_row, max_col = raw_data.shape\r\n",
        "    resolution_nm = 20\r\n",
        "    emitters_size = 8 # value * resolution_nm is the radius\r\n",
        "    emitters_grid = np.zeros((int(max_row * 100 / resolution_nm), int(max_col * 100 / resolution_nm)), dtype=int)\r\n",
        "    emitters_coordinates = []\r\n",
        "    emitters = []\r\n",
        "    emitters_cnt = 0\r\n",
        "\r\n",
        "    patch_length = 9 # Determine the patch we are going to fit to be of size [patch_length x patch length]\r\n",
        "    xy = np.zeros([2, int(patch_length ** 2)])\r\n",
        "    for i1 in range(patch_length):\r\n",
        "        for j1 in range(patch_length):\r\n",
        "            xy[:, int(i1 * patch_length + j1)] = [i1, j1]\r\n",
        "\r\n",
        "    # Fit patch to gaussian\r\n",
        "    def gauss2d(xy, offset, amp, x0, y0, sigma):\r\n",
        "        x, y = xy\r\n",
        "        return offset + amp * np.exp(-((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\r\n",
        "\r\n",
        "    for frame in range(total_frames):\r\n",
        "        img_arr = raw_data[frame, :, :]\r\n",
        "        if (np.max(img_arr) > threshold):\r\n",
        "            potential_peaks = np.where(img_arr > threshold)\r\n",
        "            for i in range(len(potential_peaks[0])):\r\n",
        "                row, col = potential_peaks[0][i], potential_peaks[1][i]\r\n",
        "\r\n",
        "                # Handle clusters in case of exceeding image shape\r\n",
        "                up = int(row + np.floor(patch_length / 2)) + 1\r\n",
        "                down = int(row - np.floor(patch_length / 2))\r\n",
        "                left = int(col - np.floor(patch_length / 2))\r\n",
        "                right = int(col + np.floor(patch_length / 2)) + 1\r\n",
        "\r\n",
        "                # Ignore out of bound blinks\r\n",
        "                if up > max_row or down < 0 or left < 0 or right > max_col:\r\n",
        "                    continue\r\n",
        "\r\n",
        "                # Ignore 1 pixel fake blinks\r\n",
        "                pixel_cnt = np.where(img_arr[down:up, left:right] > threshold)[0]\r\n",
        "                if (len(pixel_cnt) < 2):\r\n",
        "                    continue\r\n",
        "\r\n",
        "                # Initial guess\r\n",
        "                x0, y0 = int(np.floor(patch_length/2)), int(np.floor(patch_length/2))\r\n",
        "\r\n",
        "                zobs = (img_arr[down:up, left:right]).reshape(1, -1).squeeze()\r\n",
        "                guess = [np.median(img_arr), np.max(img_arr) - np.min(img_arr), x0, y0, 1]\r\n",
        "                try:\r\n",
        "                    pred_params, uncert_cov = opt.curve_fit(gauss2d, xy, zobs, p0=guess)\r\n",
        "                except:\r\n",
        "                    continue\r\n",
        "\r\n",
        "                fit = gauss2d(xy, *pred_params)\r\n",
        "\r\n",
        "                # Check if the gaussian's variance is too small or too big\r\n",
        "                if(pred_params[4] < 0.5 or pred_params[4] > 1.5):\r\n",
        "                    continue\r\n",
        "\r\n",
        "                # Taken from H. Flyvbjerg, et al. (2010) \"Optimized localization analysis for single-molecule tracking and super-resolution microscopy\"\r\n",
        "                '''sigma = pred_params[4] # Fitted sigma of the gaussian\r\n",
        "                a_squared = 1 # Pixel size\r\n",
        "                b_squared = 100/(patch_length ** 2) # Background's expected photons per pixel\r\n",
        "                N = 1 # Number of photons in a spot\r\n",
        "                sigma_a_squared = (sigma ** 2) + a_squared / 12\r\n",
        "                Variance = (sigma_a_squared / N) * (16/9 + (8*np.pi*sigma_a_squared*b_squared)/(N*a_squared))'''\r\n",
        "\r\n",
        "                # If the peak is higher than threshold proceed\r\n",
        "                curr_row = int(down + pred_params[3])\r\n",
        "                curr_col = int(left + pred_params[2])\r\n",
        "                if(curr_col > max_col or curr_col < 0 or curr_row > max_row or curr_row < 0):\r\n",
        "                    continue\r\n",
        "\r\n",
        "                # Handle same coordinate repetition\r\n",
        "                x = left + pred_params[2]\r\n",
        "                y = down + pred_params[3]\r\n",
        "                center_y = int(y * 100 / resolution_nm)\r\n",
        "                center_x = int(x * 100 / resolution_nm)\r\n",
        "\r\n",
        "                if (img_arr[curr_row, curr_col] > threshold):\r\n",
        "                    # Calculate RMS\r\n",
        "                    zobs /= np.max(zobs)\r\n",
        "                    fit /= np.max(fit)\r\n",
        "                    fit_quality = 1 - np.sqrt(np.mean((zobs - fit) ** 2))\r\n",
        "\r\n",
        "                    # If the fit quality is higher than Value > take the mean value as a new cluster's coordinates\r\n",
        "                    if (fit_quality > quality_threshold):\r\n",
        "\r\n",
        "                        # If the current pixel in the grid is already tagged for one of the emitters\r\n",
        "                        if(emitters_grid[center_y, center_x] > 0):\r\n",
        "                            # If the current blink has already been discovered\r\n",
        "                            if (emitters[emitters_grid[center_y, center_x] - 1][frame] > 0):\r\n",
        "                                continue\r\n",
        "                            else:\r\n",
        "                                # Update emitters_grid\r\n",
        "                                emitters_grid[center_y - emitters_size:center_y + emitters_size,\r\n",
        "                                              center_x - emitters_size:center_x + emitters_size] = emitters_grid[center_y, center_x]\r\n",
        "                                # Update emitters list\r\n",
        "                                emitters[emitters_grid[center_y, center_x] - 1][frame] = pred_params[0] + pred_params[1]\r\n",
        "                                continue\r\n",
        "\r\n",
        "                        # Add localization\r\n",
        "                        emitters_cnt += 1\r\n",
        "                        # Update emitters_grid\r\n",
        "                        emitters_grid[center_y-emitters_size:center_y+emitters_size,\r\n",
        "                                      center_x-emitters_size:center_x+emitters_size] = emitters_cnt\r\n",
        "                        # Update emitters list\r\n",
        "                        emitters_coordinates.append([y, x])\r\n",
        "                        time_trace = np.zeros(total_frames)\r\n",
        "                        time_trace[frame] = pred_params[0] + pred_params[1]# Offset + Amp of gaussian\r\n",
        "                        emitters.append(time_trace)\r\n",
        "\r\n",
        "    print(\"-I- found\", len(emitters), \"emitters\")\r\n",
        "\r\n",
        "    return np.array(emitters), np.array(emitters_coordinates)\r\n",
        "\r\n",
        "def find_trajectories(raw_data, threshold, MaxTraceLength):\r\n",
        "    \"\"\"\r\n",
        "        Finds trajectories from the given TIFF video\r\n",
        "        :param raw_data: Tensor [# of trajectories, trajectories length]\r\n",
        "        :param threshold: Float for the minimal threshold for peak detection\r\n",
        "        :return: List of all the different trajectories in time.\r\n",
        "    \"\"\"\r\n",
        "    dict_vec = []\r\n",
        "    dist = 5\r\n",
        "    total_frames, max_row, max_col = raw_data.shape\r\n",
        "\r\n",
        "    for frame in range(total_frames):\r\n",
        "        img_arr = raw_data[frame, :, :]\r\n",
        "        if (np.max(img_arr) > threshold):\r\n",
        "            potential_peaks = np.where(img_arr > threshold)\r\n",
        "\r\n",
        "            for i in range(len(potential_peaks[0])):\r\n",
        "                row, col = potential_peaks[0][i], potential_peaks[1][i]\r\n",
        "                up = row + 1 if row < max_row else row\r\n",
        "                down = row - 1 if row > 0 else row\r\n",
        "                left = col - 1 if col > 0 else col\r\n",
        "                right = col + 1 if col < max_col else col\r\n",
        "\r\n",
        "                if (np.mean(img_arr[down:up+1, left:right+1]) > threshold):\r\n",
        "                    flag = False\r\n",
        "                    for k in range(len(dict_vec)):\r\n",
        "                        if (row == dict_vec[k]['row'] and col == dict_vec[k]['col'] and\r\n",
        "                                np.abs(frame - dict_vec[k]['start']) < MaxTraceLength):\r\n",
        "                            flag = True\r\n",
        "                            break\r\n",
        "                        else:\r\n",
        "                            if(np.abs(row - dict_vec[k]['row']) < dist and np.abs(col - dict_vec[k]['col']) < dist and\r\n",
        "                                    np.abs(frame - dict_vec[k]['start']) < MaxTraceLength):\r\n",
        "                                start = dict_vec[k]['start'] if dict_vec[k]['start'] < total_frames else total_frames - MaxTraceLength\r\n",
        "                                rowK = dict_vec[k]['row']\r\n",
        "                                colK = dict_vec[k]['col']\r\n",
        "                                trace1 = raw_data[start:(start+MaxTraceLength), row, col]\r\n",
        "                                trace2 = raw_data[start:(start+MaxTraceLength), rowK, colK]\r\n",
        "                                corr = np.corrcoef(trace1, trace2)[0, 1]\r\n",
        "                                if(np.abs(corr) > 0.1):\r\n",
        "                                    flag = True\r\n",
        "                                    break\r\n",
        "\r\n",
        "                    if (flag == False):\r\n",
        "                        dict = {'start': frame, 'row': row, 'col': col}\r\n",
        "                        dict_vec.append(dict)\r\n",
        "\r\n",
        "    N = len(dict_vec)\r\n",
        "    trajectories_arr = np.zeros((N, MaxTraceLength))\r\n",
        "    coordinates = np.zeros((N, 2))\r\n",
        "    for i in range(N):\r\n",
        "        start = dict_vec[i]['start']\r\n",
        "        stop = dict_vec[i]['start']+MaxTraceLength\r\n",
        "        if(stop > total_frames):\r\n",
        "            stop = total_frames\r\n",
        "            start = total_frames - MaxTraceLength\r\n",
        "        trajectories_arr[i, :] = raw_data[start:stop, dict_vec[i]['row'], dict_vec[i]['col']]\r\n",
        "        coordinates[i, 0] = dict_vec[i]['row']\r\n",
        "        coordinates[i, 1] = dict_vec[i]['col']\r\n",
        "\r\n",
        "    print(\"-I- found\", N, \"time traces\")\r\n",
        "    return trajectories_arr, coordinates\r\n",
        "\r\n",
        "def feature_extraction(trajectories, threshold, numOfBins):\r\n",
        "    \"\"\"\r\n",
        "        Extracting n_blinks features according to the given trajectories\r\n",
        "        :param trajectories: List of trajectories\r\n",
        "        :param threshold: Float for minimal threshold for peak detection\r\n",
        "        :param numOfBins: Int for the range of the n_blinks histogram\r\n",
        "        :param tau: Int for the tau_c parameter\r\n",
        "        :return: Tensor [1, numOfBins] for the n_blinks histogram of the experiment.\r\n",
        "    \"\"\"\r\n",
        "    L = len(trajectories)\r\n",
        "    features = np.zeros((L, numOfBins))\r\n",
        "    for i in range(L):\r\n",
        "        numOfTrajectories, TrajectoryLength = trajectories[i].shape\r\n",
        "        for trajectory in range(numOfTrajectories):\r\n",
        "            n_blinks = 0\r\n",
        "            frame = 0\r\n",
        "            while(frame < TrajectoryLength):\r\n",
        "                if(trajectories[i][trajectory, frame] >= threshold):\r\n",
        "                    n_blinks += 1\r\n",
        "                    start_time = frame + 10\r\n",
        "                    while (trajectories[i][trajectory, frame] >= threshold and frame < start_time):\r\n",
        "                        frame += 1\r\n",
        "                        if (frame > TrajectoryLength - 1):\r\n",
        "                            break\r\n",
        "                else:\r\n",
        "                    if(trajectories[i][trajectory, frame] < threshold/2):\r\n",
        "                        while (trajectories[i][trajectory, frame] < threshold):\r\n",
        "                            frame += 1\r\n",
        "                            if (frame > TrajectoryLength - 1):\r\n",
        "                                break\r\n",
        "                    else:\r\n",
        "                        frame += 1\r\n",
        "\r\n",
        "            if (n_blinks > 0 and n_blinks < numOfBins):\r\n",
        "                features[i, n_blinks - 1] += 1\r\n",
        "\r\n",
        "    np.save('X_test', features)\r\n",
        "    return features\r\n",
        "\r\n",
        "def LoadFinalDataSet():\r\n",
        "    \"\"\"\r\n",
        "        Loads an existing data set after feature extraction\r\n",
        "        :return: data set [# of experiments, numOfBins].\r\n",
        "    \"\"\"\r\n",
        "    data_set = np.load('X_test.npy')\r\n",
        "    return data_set\r\n",
        "\r\n",
        "def CreateSimulatedDataSet_special():\r\n",
        "    path = 'D:\\Project\\data'\r\n",
        "    files = [r'CTLA4/mEos3.2.tif', r'CTLA4/mEos3.2_X2.tif']\r\n",
        "    data = np.zeros((24000, 251, 257))\r\n",
        "\r\n",
        "    for j in range(2):\r\n",
        "        tiff = tc.opentiff(os.path.join(path, files[j]))\r\n",
        "        for i in range(12000):\r\n",
        "            data[int(12000*j + i), :, :] = tiff.find_and_read(i)\r\n",
        "\r\n",
        "    return data\r\n",
        "\r\n",
        "def CreateSimulatedDataSet(data_set_size, num_of_molecules, bleach_proba, numOfBins):\r\n",
        "    \"\"\"\r\n",
        "        Creates many simulations of features based on ground truth\r\n",
        "        :param data_set_size: Int for the number of simulations to perform\r\n",
        "        :param num_of_molecules: Int for the number of molecules to simulate per simulations\r\n",
        "        :param bleac_proba: Float for the bleaching probability of single cluster\r\n",
        "        :param numOfBins: Int for the range of the n_blinks histogram\r\n",
        "        :return: data_set [data_set_size, numOfBins] for the entire simulated data set.\r\n",
        "    \"\"\"\r\n",
        "    X = np.zeros([data_set_size, numOfBins])\r\n",
        "    y = np.zeros(data_set_size)\r\n",
        "\r\n",
        "    for i in range(data_set_size):\r\n",
        "        if(np.random.rand() > 0.4): # Creating more monomers only experiments\r\n",
        "            y[i] = 0 # Monomers only\r\n",
        "        else:\r\n",
        "            y[i] = np.random.rand() # Random dimers percentage\r\n",
        "\r\n",
        "        MonoList = []\r\n",
        "        for j in range(int(num_of_molecules - num_of_molecules * y[i])): # MONOMERS\r\n",
        "            MonoCnt = 1 # Always start with one blink\r\n",
        "            while(np.random.rand() > bleach_proba): # add blinks until bleaches\r\n",
        "                MonoCnt += 1\r\n",
        "            MonoList.append(MonoCnt)\r\n",
        "        DimersList = []\r\n",
        "        for j in range(int(num_of_molecules - num_of_molecules * y[i]), num_of_molecules): # DIMERS\r\n",
        "            DimersCnt = 2 # Always start with two blinks\r\n",
        "            while(np.random.rand() > bleach_proba): # add blinks until bleaches\r\n",
        "                DimersCnt += 1\r\n",
        "            while (np.random.rand() > bleach_proba): # dimers bleach twice\r\n",
        "                DimersCnt += 1\r\n",
        "            DimersList.append(DimersCnt)\r\n",
        "\r\n",
        "        Nblinks_mono = np.histogram(np.array(MonoList) - 1, bins=numOfBins, range=(0, numOfBins))[0]\r\n",
        "        Nblinks_dimers = np.histogram(np.array(DimersList) - 1, bins=numOfBins, range=(0, numOfBins))[0]\r\n",
        "\r\n",
        "        X[i, :] = Nblinks_mono + Nblinks_dimers\r\n",
        "\r\n",
        "    np.save('X', X)\r\n",
        "    np.save('y', y)\r\n",
        "    return X, y\r\n",
        "\r\n",
        "def LoadSimulatedDataSet_special():\r\n",
        "    \"\"\"\r\n",
        "        Loads an existing trajectories list\r\n",
        "        :return: List of trajectories.\r\n",
        "    \"\"\"\r\n",
        "    X = np.load('X_special.npy')\r\n",
        "    y = np.load('y_special.npy')\r\n",
        "    return X, y\r\n",
        "\r\n",
        "def LoadSimulatedDataSet():\r\n",
        "    \"\"\"\r\n",
        "        Loads an existing trajectories list\r\n",
        "        :return: List of trajectories.\r\n",
        "    \"\"\"\r\n",
        "    X = np.load('X.npy')\r\n",
        "    y = np.load('y.npy')\r\n",
        "    return X, y\r\n",
        "\r\n",
        "def Filter_beads(data):\r\n",
        "    mean_intensity = np.mean(data)\r\n",
        "    std_intensity = np.std(data)\r\n",
        "    bids_loc = np.where(np.mean(data, axis=0) > mean_intensity * 1.1)\r\n",
        "    data[:, bids_loc[0], bids_loc[1]] = np.random.normal(mean_intensity, std_intensity, [data.shape[0], len(bids_loc[0])])\r\n",
        "    return data\r\n",
        "\r\n",
        "def CreateDataLoader(X, y, batch_size):\r\n",
        "    \"\"\"\r\n",
        "        Creates data loader for X, y pairs\r\n",
        "        :param X: Tensor [# of observations, numOfBins] for the observations\r\n",
        "        :param y: Tensor [# of observations] for the predictions\r\n",
        "        :param batch_size: Int for batch size\r\n",
        "        :return: data loader for training/ testing routines\r\n",
        "    \"\"\"\r\n",
        "    data_loader = torch.utils.data.DataLoader(TensorDataset(X, y), batch_size=batch_size)\r\n",
        "    return data_loader\r\n",
        "\r\n",
        "print('--------------------------------')\r\n",
        "print('DeepSTORM installation complete.')\r\n",
        "\r\n",
        "# Exporting requirements.txt for local run\r\n",
        "!pip freeze > requirements.txt\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2DKywvgI5F0"
      },
      "source": [
        "Parameters Initialization and Setup\r\n",
        "In the next block we will import the relevant packages for QAFKA. Then, we will define some important parameters for our run.\r\n",
        "\r\n",
        "**numOfBins** - Specifies the number of bins in the histogram of the number of blinking events (default: 20)\r\n",
        "\r\n",
        "**chop** - Specifies which frames of the experiment you would like to analyze. For example: chop = [0, 1000] will cause QAFKA to analyze the experiment between the first frame and the 1000th frame.\r\n",
        "\r\n",
        "**pixel_length** - Specifies the experiment's pixel size [nm]\r\n",
        "\r\n",
        "**scale_size** - Specifies the resolution scaling. For example: scale_size = 3 and pixel_length = 150 [nm] would results in a reconstrcuted image with grid size of 50 [nm].\r\n",
        "\r\n",
        "**mergeing_radius** - Specifies the emitters merging radius for detection. If two clusters would be located within this radius they would be considered as a single cluster.\r\n",
        "\r\n",
        "**numOfClusters** - Specifies the number of simulated clusters in each simulated experiment\r\n",
        "\r\n",
        "**dataPath** - Specifies the path to the data.\r\n",
        "\r\n",
        "**file_names** - Specifies the names of the TIFF files (at least one experiment is required). For example: 'first_exp.tif'.\r\n",
        "\r\n",
        "**qualityThreshold** - Specifies the minimal fitting score for the localization block. (default: 0.85)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKPTWPctIkRO"
      },
      "source": [
        "numOfBins = 20\r\n",
        "chop = [0, 12000]\r\n",
        "pixel_length = 150 #[nm]\r\n",
        "scale_size = 3\r\n",
        "merging_radius = 150 #[nm]\r\n",
        "file_names = [r'Ligand/unstim/200327_METmEos4b_C6_unstim_S3_cell2_568.tif']\r\n",
        "qualityThreshold = 0.85"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY30xNWtIkxN"
      },
      "source": [
        "## Run Configuration\r\n",
        "**LoadData** - Determines if we want to load new experimental data (True) or we want to use an already loaded data (False).\r\n",
        "\r\n",
        "**FilterBeads** - Determines if an additional beads filtration algorithm is needed for the experimental data.\r\n",
        "\r\n",
        "**CreateSimulatedData** - Determines if we want to use the same training data as before (True) or we want to create new training set (False).\r\n",
        "\r\n",
        "**TrainNet** - Determines if we want to train the neural network (True) or not (False).\r\n",
        "\r\n",
        "**preTrainedModelPath** - Specifies the pre-trained model to load in case we do not want to train the net. (the training runtime is typically 15-30 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFyK1_xGIk-S"
      },
      "source": [
        "LoadData = True #@param {type:\"boolean\"}\r\n",
        "FilterBeads = False #@param {type:\"boolean\"}\r\n",
        "CreateSimulatedData = True #@param {type:\"boolean\"}\r\n",
        "TrainNet = True #@param {type:\"boolean\"}\r\n",
        "preTrainedModel = '' #@param {type: \"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRbMWCGjJ6f6"
      },
      "source": [
        "## Analysis Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPUPRj6jJ7JQ"
      },
      "source": [
        "max_size = int((chop[1]-chop[0])/1000)\r\n",
        "resolution_nm = pixel_length/scale_size #[nm]\r\n",
        "\r\n",
        "if(CreateData):\r\n",
        "    trajectories, clusterCoordinations = [], []\r\n",
        "    for i, file in enumerate(file_names):\r\n",
        "        print(\"**** Analyzing Tiff number {} ****\".format(i+1))\r\n",
        "        # Load TIFF files and create data_set\r\n",
        "        Data_Set = CreateDataSet(file, chop)\r\n",
        "        \r\n",
        "        # Segment the experiment before and after laser activation\r\n",
        "        seg = segment(Data_Set, threshold=0.15, window_size=100)\r\n",
        "        \r\n",
        "        # Filter beads (if True)\r\n",
        "        if(FilterBeads):\r\n",
        "            Data_Set = Filter_beads(Data_Set)\r\n",
        "        \r\n",
        "        # Background noise cleaning\r\n",
        "        Data_Set = clean_bg_noise(Data_Set, patch_length=5)\r\n",
        "        \r\n",
        "        # Clusters localization\r\n",
        "        Max_Data_Set = CreateMaxDataSet(Data_Set, max_size, seg)\r\n",
        "        DataThreshold, MaxThreshold = calc_threshold(Data_Set, Max_Data_Set)\r\n",
        "        coordinates = LocalizeEmitters(Max_Data_Set, MaxThreshold, quality_threshold, pixel_length, resolution_nm, emitters_size)\r\n",
        "        \r\n",
        "        # Create time traces for each cluster\r\n",
        "        timeTraces = ExtractTimeTraces(Data_Set[seg:, :, :], coordinates, pixel_length, resolution_nm, quality_threshold, DataThreshold, emitters_size)\r\n",
        "        \r\n",
        "        # Save the time traces and clusters locations of all experiments in a list\r\n",
        "        trajectories.append(timeTraces)\r\n",
        "        clusterCoordinations.append(coordinates)\r\n",
        "        \r\n",
        "        # The coordinations file would be saved as 'coordinated.npy'\r\n",
        "        np.save('coordinates', coordinates)\r\n",
        "\r\n",
        "    # Extract the features that would serve as the neural network's input\r\n",
        "    X_test = feature_extraction(trajectories, DataThreshold, numOfBins)\r\n",
        "else:\r\n",
        "    # Load features of an already analyzed experiment\r\n",
        "    X_test = LoadFinalDataSet()\r\n",
        "\r\n",
        "print(\"-I- Experimental Data was loaded successfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmbn0CsuKBsg"
      },
      "source": [
        "## Visualize Localizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sHC-8TIKB0i"
      },
      "source": [
        "# This will plot a max projection image of the last experiment with the localization marked on it\r\n",
        "debug_entire_exp(Max_Data_Set, coordinates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5NZhYaRKJhM"
      },
      "source": [
        "## Simulated Data Setup\r\n",
        "If you chose to simulate the training data, you would need to specify the following parameters:\r\n",
        "\r\n",
        "**numOfClusters** - Specifies the number of simulated clusters in each simulation (relevant only if CreateSimulatedData is set to True).\r\n",
        "\r\n",
        "**bleach_proba** - Specifies the bleaching probability of the used fluorophore.\r\n",
        "\r\n",
        "**TrainSetSize** - Specifies the number of simulated experiments to be created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f73DtIEcKJoU"
      },
      "source": [
        "numOfClusters = 200 #@param {type:\"number\"}\r\n",
        "bleach_proba = 0.41 #@param {type:\"number\"}\r\n",
        "TrainSetSize = 10000 #@param {type:\"number\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMRjqS7CKSQr"
      },
      "source": [
        "## Create Simulated Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5og_72yCKSXa"
      },
      "source": [
        "if(CreateSimulatedData):\r\n",
        "    [X, y] = CreateSimulatedDataSet(TrainSetSize, num_of_molecules, bleach_proba, numOfBins)\r\n",
        "else:\r\n",
        "    [X, y] = LoadSimulatedDataSet()\r\n",
        "\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.75)\r\n",
        "[X_train, X_val, X_test] = Normalization(X_train, X_val, X_test)\r\n",
        "[X_train, X_val, X_test] = BiasTrick(X_train, X_val, X_test)\r\n",
        "y_train = torch.FloatTensor(y_train)\r\n",
        "y_val = torch.FloatTensor(y_val)\r\n",
        "\r\n",
        "print(\"-I- Simulated Data was created successfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-mvAtxXKWZ5"
      },
      "source": [
        "## Build Model\r\n",
        "In the next block we will build the neural network model.\r\n",
        "\r\n",
        "**lr** - Specifies the training phase learning rate.\r\n",
        "\r\n",
        "**betas** - Specifies the parameters for ADAM optimizer.\r\n",
        "\r\n",
        "**batch_size** - Specifies the batch size of the training phase.\r\n",
        "\r\n",
        "**epochs** - Specifies the maximal training epoch.\r\n",
        "\r\n",
        "**early_stopping** - Specifies the tolerance of the neural network to lack of improvement in the validation loss. For example: early_stopping = 5, would stop the trainig phase if the validation loss did not improve for 5 epochs.\r\n",
        "\r\n",
        "**Model_output_dir** - Specifies where the final model will be saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYi12vj4KWhs"
      },
      "source": [
        "lr = 1e-5 #@param {type:\"number\"}\r\n",
        "batch_size = 4 #@param {type:\"number\"}\r\n",
        "epochs = 1000 #@param {type:\"number\"}\r\n",
        "Model_output_dir = '' #@param {type: \"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCwSwYw5PnPA"
      },
      "source": [
        "### Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YARN67VZPiDi"
      },
      "source": [
        "betas = (0.99, 0.999)\r\n",
        "early_stopping = np.min((int(epochs/5), 15))\r\n",
        "model = CustomNet(torch.numel(X_train[0]), [128, 128, 128, 128])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2vHyyrSKccF"
      },
      "source": [
        "## Training Phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2agi6BRHKcip"
      },
      "source": [
        "if(TrainNet):\r\n",
        "    criterion = torch.nn.MSELoss()\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\r\n",
        "\r\n",
        "    dl_train = CreateDataLoader(X_train, y_train, batch_size=batch_size)\r\n",
        "    dl_val = CreateDataLoader(X_val, y_val, batch_size=1)\r\n",
        "\r\n",
        "    # ================= Train Net ================\r\n",
        "    trainer = Trainer(model, criterion, optimizer)\r\n",
        "    trainer.fit(dl_train, dl_val, num_epochs=epochs, early_stopping=early_stopping, print_every=1)\r\n",
        "    torch.save(trainer.model.state_dict(), os.path.join(Model_output_dir, 'final_model_gauss'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvYMkG1PKhRm"
      },
      "source": [
        "## Load Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZlVjvxNKhaV"
      },
      "source": [
        "model.load_state_dict(torch.load(preTrainedModel))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSvxQClBKm5Q"
      },
      "source": [
        "## Testing Phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmSznERqKnCd"
      },
      "source": [
        "y_val_pred = model(X_val)\r\n",
        "y_test_pred = model(X_test).squeeze()\r\n",
        "y_test_pred = torch.max(y_test_pred, torch.zeros(y_test_pred.shape))\r\n",
        "\r\n",
        "val_acc = torch.mean(torch.abs(y_val_pred.squeeze() - y_val))\r\n",
        "print(\"Neural Network Validation MSE:\", 100 * val_acc.item())\r\n",
        "\r\n",
        "print(\"Dimers Percentage Predictions Per Experiment:\")\r\n",
        "if(y_test_pred.shape == torch.Size([])):\r\n",
        "    print(\"0:\", 100 * y_test_pred.item())\r\n",
        "else:\r\n",
        "    for i in range(y_test_pred.shape[0]):\r\n",
        "        print(\"i:\", 100 * y_test_pred[i].item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zCmvi7QQbXg"
      },
      "source": [
        "#**Thank you for using QAFKA!**"
      ]
    }
  ]
}